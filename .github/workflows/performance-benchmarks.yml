name: Performance Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read
      pull-requests: write  # Required to comment on PRs
      issues: write  # Required to create comments

    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0  # Full history for git commit tracking

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install pytest pytest-asyncio pyyaml

      - name: Run performance benchmarks
        id: benchmarks
        run: |
          mkdir -p reports
          pytest tests/performance/test_comprehensive_benchmarks.py::test_generate_full_report -v -s --basetemp=reports
        continue-on-error: true

      - name: Copy benchmark reports to expected location
        if: always()
        run: |
          # Find and copy benchmark reports from pytest temp directory
          find reports -name "latest_benchmark.json" -exec cp {} reports/performance/ \; || true
          find reports -name "*.html" -path "*/performance/*" -exec cp {} reports/performance/ \; || true
          find reports -name "benchmark_history.jsonl" -exec cp {} reports/performance/ \; || true
          ls -la reports/performance/ || echo "No reports found"

      - name: Check for regressions
        id: regression_check
        run: |
          python scripts/check_performance_regression.py || echo "regression_detected=false" >> $GITHUB_OUTPUT
        continue-on-error: false

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-benchmarks-${{ github.sha }}
          path: |
            reports/performance/latest_benchmark.json
            reports/performance/latest_benchmark.html
            reports/performance/benchmark_history.jsonl
          retention-days: 90

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        continue-on-error: true  # Don't fail if PR commenting fails (permissions)
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const reportPath = 'reports/performance/latest_benchmark.json';

            if (!fs.existsSync(reportPath)) {
              console.log('No benchmark report found');
              return;
            }

            const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
            const results = report.results || [];

            const passed = results.filter(r => r.passed).length;
            const total = results.length;
            const passRate = total > 0 ? (passed / total * 100).toFixed(1) : 0;

            let comment = `## ðŸ“Š Performance Benchmark Results\n\n`;
            comment += `**Commit**: ${report.git_commit || 'unknown'}\n`;
            comment += `**Timestamp**: ${report.timestamp}\n`;
            comment += `**Pass Rate**: ${passed}/${total} (${passRate}%)\n\n`;

            comment += `| Benchmark | P50 | P95 | P99 | Target | Status |\n`;
            comment += `|-----------|-----|-----|-----|--------|--------|\n`;

            results.forEach(r => {
              const status = r.passed ? 'âœ… PASS' : 'âŒ FAIL';
              const target = r.target_ms ? r.target_ms.toFixed(1) : 'N/A';
              comment += `| ${r.name} | ${r.median_ms.toFixed(2)}ms | ${r.p95_ms.toFixed(2)}ms | ${r.p99_ms.toFixed(2)}ms | ${target}ms | ${status} |\n`;
            });

            comment += `\n[View HTML Report](../../actions/runs/${{ github.run_id }})\n`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Fail on regression
        if: steps.regression_check.outputs.regression_detected == 'true'
        run: |
          echo "âŒ Performance regression detected!"
          exit 1

      - name: Publish results to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./reports/performance
          destination_dir: performance/${{ github.sha }}
          keep_files: true

  # Compare with baseline
  compare:
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: performance-benchmarks-${{ github.sha }}
          path: ./current

      - name: Download baseline
        uses: dawidd6/action-download-artifact@v6
        with:
          workflow: performance-benchmarks.yml
          branch: main
          name: performance-benchmarks-*
          path: ./baseline
          search_artifacts: true
        continue-on-error: true

      - name: Compare results
        continue-on-error: true  # May not have baseline on first run
        run: |
          python scripts/compare_performance.py ./baseline ./current > comparison.md

      - name: Post comparison
        if: github.event_name == 'pull_request'
        continue-on-error: true  # Don't fail if comparison posting fails
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('comparison.md')) {
              const comparison = fs.readFileSync('comparison.md', 'utf8');
              if (comparison && comparison.trim().length > 0) {
                await github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comparison
                });
                console.log('Posted performance comparison to PR');
              } else {
                console.log('Comparison file is empty, skipping comment');
              }
            } else {
              console.log('No comparison file found, skipping comment');
            }
