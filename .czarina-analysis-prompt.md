# Czarina Project Analysis Template

**Purpose:** AI-driven analysis of implementation plans to generate optimal orchestration setup.

---

## Analysis Instructions

You are analyzing an implementation plan to recommend optimal Czarina orchestration using **version-based planning with token metrics** (NOT time-based).

### CRITICAL RULES

**❌ NEVER:**
- Use time estimates (weeks, days, sprints, quarters)
- Use calendar dates
- Use sprint numbers
- Reference time-based milestones

**✅ ALWAYS:**
- Use semantic versions (v0.1.0, v0.2.0, v1.0.0)
- Use phases for large features (v0.2.1-phase1, v0.2.1-phase2)
- Use token budgets (projected and recorded)
- Calculate efficiency ratios

---

## Input Plan

```
# SARK v1.3.1 Implementation Plan

**Goal**: Technical debt reduction, audit preparation, and v1.3.0 release finalization

**Current State**:
- Test coverage: 6.04% (target: 85%)
- 134 modules with 0% coverage
- Tests disabled in CI (153 pre-existing failures)
- v1.3.0 features complete but not formally released

**Strategy**: Phased approach prioritizing audit readiness and quick wins

---

## Phase 1: Quick Wins & Release (2-3 days)

### 1.1 Fix Version Mismatch (30 min)
**File**: `src/sark/__init__.py`
- Change `__version__ = "1.1.0"` → `"1.3.0"`
- Verify alignment with `pyproject.toml`

### 1.2 Finalize v1.3.0 Release Notes (1 hour)
**File**: `docs/v1.3.0/RELEASE_NOTES.md`
- Set release date (currently "TBD")
- Review feature descriptions
- Add upgrade instructions

### 1.3 Create GitHub Release (1 hour)
- Tag: `v1.3.0` at commit `c8a2dc6`
- Attach performance benchmarks from `reports/PERFORMANCE_BENCHMARKS.md`
- Publish as latest release

### 1.4 v1.3.0 Migration Guide (4-6 hours)
**New File**: `docs/v1.3.0/MIGRATION_GUIDE.md`
**Pattern**: Follow `docs/MIGRATION_GUIDE.md` structure

Sections:
- Migration Overview (v1.2.0 → v1.3.0)
- Pre-Migration Checklist
- Feature Comparison Table
- Zero-Downtime Deployment Strategy
- Step-by-Step Migration Procedure
  - Prompt injection detection setup
  - Anomaly detection configuration
  - Network policies deployment
  - Secret scanning activation
  - MFA configuration
- Verification Procedures
- Rollback Plan
- Troubleshooting

### 1.5 Update Documentation Navigation (30 min)
**File**: `mkdocs.yml`

Add to nav section:
```yaml
- v1.3.0 Release:
  - Release Notes: v1.3.0/RELEASE_NOTES.md
  - Migration Guide: v1.3.0/MIGRATION_GUIDE.md
```

**Success Criteria**:
- ✅ Version consistency across codebase
- ✅ v1.3.0 GitHub release published
- ✅ Migration guide created following established patterns
- ✅ Documentation indexed in mkdocs

---

## Phase 2: Integration Test Infrastructure (3-5 days)

**Goal**: Fix async fixture scoping, eliminate collection errors, enable CI tests

### 2.1 Fix Docker Fixture Scoping (4-6 hours)
**File**: `tests/fixtures/integration_docker.py`

**Problem**: Session-scoped sync fixtures with function-scoped async fixtures
- `postgres_service`, `redis_service`, `opa_service` (session, sync)
- `postgres_connection`, `redis_connection` (function, async) → depends on session

**Solution**:
- Change `postgres_connection` (line 84) to `scope="session"`
- Change `timescaledb_connection` (line 148) to `scope="session"`
- Change `redis_connection` (line 212) to `scope="session"`
- Add function-scoped cleanup fixtures for test isolation

### 2.2 Create Missing Docker Compose (3-4 hours)
**New File**: `tests/fixtures/docker-compose.integration.yml`

Required services:
- PostgreSQL (port 5432)
- TimescaleDB (port 5433, different from PostgreSQL)
- Valkey (port 6379)
- OPA (port 8181)
- gRPC mock server (ports 4770, 4771)

### 2.3 Fix Test Collection Errors (6-8 hours)
**Current**: 84 collection errors out of 221 tests

Approach:
1. Run `pytest --collect-only` to identify errors
2. Group by error type (import, fixture, syntax)
3. Fix systematically by type
4. May need to skip some v2.0 tests

### 2.4 Consolidate Fixture Documentation (2-3 hours)
**New File**: `tests/fixtures/README.md`

Document:
- Fixture hierarchy (root conftest → integration → Docker)
- When to use each fixture level
- How to opt into Docker fixtures
- Scope best practices

### 2.5 Re-enable CI Tests (2-3 hours)
**File**: `.github/workflows/ci.yml`

Replace lines 53-58 (current skip) with:
```yaml
- name: Run unit tests
  run: |
    pytest -m "not integration and not k8s and not slow" \
           --cov=src --cov-report=xml --cov-report=term

- name: Run integration tests (non-blocking)
  run: |
    pytest -m "integration" || true
  continue-on-error: true
```

**Success Criteria**:
- ✅ 0 test collection errors
- ✅ Fixture scope warnings resolved
- ✅ CI tests enabled
- ✅ Docker integration tests runnable locally

---

## Phase 3: Test Coverage Improvement (10-15 days)

**Goal**: Increase coverage from 6% → 85%
**Strategy**: Prioritize high-value, zero-coverage modules

### Workstream 3A: Policy & Security (4-5 days)

1. **Policy Cache** (259 statements, 0% → 85%)
   - New file: `tests/services/policy/test_cache.py`
   - Tests: Cache hit/miss, TTL, batch ops, Valkey failures
   - ~20-25 tests

2. **OPA Client** (216 statements, 0% → 85%)
   - New file: `tests/services/policy/test_opa_client.py`
   - Tests: Evaluation, batching, errors, retries, health
   - ~18-22 tests

3. **MFA System** (245 statements, 56% → 85%)
   - Enhance: `tests/security/test_mfa.py`
   - Tests: TOTP edge cases, SMS/push, expiration, rate limiting
   - +10-15 tests

4. **Anomaly Alerts** (151 statements, 51% → 85%)
   - New/enhance: `tests/security/test_anomaly_alerts.py`
   - Tests: Alert integration, baseline calc, auto-suspend
   - +15-20 tests

5. **Security Config** (91 statements, 63% → 85%)
   - Enhance existing test
   - Tests: Validation, defaults, edge cases
   - +8-10 tests

### Workstream 3B: Configuration (2-3 days)

1. **Settings Module** (209 statements, 0% → 85%)
   - New file: `tests/config/test_settings.py`
   - Tests: Env loading, validation, defaults, JWT/LDAP/OIDC config
   - ~25-30 tests

### Workstream 3C: Adapters (4-5 days)

1. **MCP Adapter** (236 statements, 0% → 85%)
   - Enhance: `tests/adapters/test_mcp_adapter.py`
   - Tests: Tool invocation, serialization, errors, connection
   - ~20-25 tests

2. **gRPC Adapter** (226 statements, 0% → 85%)
   - Enhance: `tests/adapters/test_grpc_adapter.py`
   - Tests: Channel mgmt, RPC calls, TLS, deadlines
   - ~18-22 tests

3. **HTTP Adapter** (217 statements, 0% → 85%)
   - Enhance: `tests/adapters/test_http_adapter.py`
   - Tests: HTTP methods, auth headers, retries, timeouts
   - ~18-22 tests

### Workstream 3D: API Routers (3-4 days)

1. **Auth Router** (0% → 85%)
   - New file: `tests/api/routers/test_auth.py`
   - Tests: Login, refresh, logout, password reset, provider integration
   - ~25-30 tests

2. **Policy Router** (0% → 85%)
   - New file: `tests/api/routers/test_policy.py`
   - Tests: CRUD, evaluation, authorization, batch ops
   - ~20-25 tests

3. **API Keys Router** (0% → 85%)
   - New file: `tests/api/routers/test_api_keys.py`
   - Tests: Creation, rotation, revocation, scoping
   - ~15-20 tests

4. **Gateway Router** (0% → 85%)
   - New file: `tests/api/routers/test_gateway.py`
   - Tests: MCP operations, tool invocation
   - ~20-25 tests

5. **SAML Router** (0% → 85%)
   - New file: `tests/api/routers/test_saml.py`
   - Tests: SSO flows, ACS, SLO
   - ~15-18 tests

### Workstream 3E: Auth Services (3-4 days)

1. **JWT Handler** (0% → 85%)
   - New file: `tests/services/auth/test_jwt.py`
   - Tests: Token creation (HS256/RS256), validation, expiration
   - ~20-25 tests

2. **API Key Service** (0% → 85%)
   - New file: `tests/services/auth/test_api_key.py`
   - Tests: Generation, validation, rotation
   - ~15-18 tests

3. **Session Management** (0% → 85%)
   - New files: `tests/services/auth/test_session.py`, `test_sessions.py`
   - Tests: Creation, refresh, revocation, cleanup
   - ~20-25 tests per file

4. **Auth Providers** (0% → 85%)
   - New files for OIDC, SAML, LDAP providers
   - Tests: Provider-specific flows, error handling
   - ~15-20 tests per provider

**Best Practices**:
- Use existing fixtures from `tests/conftest.py` (`mock_redis`, `db_session`, `opa_client`)
- Follow `asyncio_mode = "auto"` for async tests
- Use pytest-httpx for HTTP mocking
- AAA pattern (Arrange, Act, Assert)
- Factories for test data

**Success Criteria**:
- ✅ Overall coverage ≥ 85%
- ✅ No critical modules < 70%
- ✅ Auth & policy modules ≥ 90%
- ✅ Tests run in < 5 min (unit)

---

## Phase 4: Audit Documentation (2-3 days)

### 4.1 Security Evidence Collection (4-6 hours)
**New File**: `docs/v1.3.0/SECURITY_EVIDENCE.md`

Compile:
- Authentication flow diagrams
- Authorization decision trees
- Security control matrix (OWASP Top 10 mapping)
- Encryption documentation (at rest/in transit)
- Secret management procedures
- Network architecture diagrams
- Access control documentation

### 4.2 Dependency Audit Report (2-3 hours)
**New File**: `docs/v1.3.0/DEPENDENCY_AUDIT.md`

- Run `pip-audit` on all dependencies
- Document known CVEs and mitigations
- Update schedule for dependency patches

### 4.3 Test Coverage Audit Report (2-3 hours)
**New File**: `docs/v1.3.0/TEST_COVERAGE_AUDIT.md`

- Coverage metrics by module
- Testing strategy explanation
- Rationale for any modules < 85%
- Security test scenarios covered
- Generate HTML coverage report

### 4.4 Configuration Security Review (3-4 hours)
**New File**: `docs/v1.3.0/CONFIGURATION_SECURITY.md`

- Document all configuration options
- Security implications per setting
- Recommended production values
- Dangerous configurations to avoid

### 4.5 Enhanced Audit Runbook (3-4 hours)
**Enhance**: `docs/v1.3.0/SECURITY_AUDIT_PREP.md` (from Phase 1)

Add:
- v1.3.0 security feature review procedures
- Test execution procedures for auditors
- Code review checklist
- Vulnerability scanning procedures

**Success Criteria**:
- ✅ Complete security evidence package
- ✅ Dependency audit with mitigation plan
- ✅ Test coverage documented
- ✅ Configuration security guide
- ✅ Auditors can independently assess security

---

## Critical Path & Dependencies

```
Phase 1 (Quick Wins)
    ↓
Phase 2 (Test Infrastructure) ← CRITICAL BLOCKER
    ↓
Phase 3 (Workstreams A-E in parallel)
    ↓
Phase 4 (Audit Docs)
```

**Parallelization**:
- All Phase 1 tasks can run in parallel
- All Phase 3 workstreams can run in parallel
- Phase 4 can start before Phase 3 completes (except test coverage report)

**Alternative Execution for Audit Urgency**:
1. Phase 1 (Quick Wins) - 2-3 days
2. Phase 4 (Audit Prep) - 2-3 days ← Do this if audit is imminent
3. Phase 2 (Test Infra) - 3-5 days
4. Phase 3 (Coverage) - 10-15 days ← Can continue post-audit

---

## Critical Files

**Highest Priority (Phase 1 & 4)**:
- `src/sark/__init__.py` - Version fix (1 line change)
- `docs/v1.3.0/RELEASE_NOTES.md` - Finalize date
- `docs/v1.3.0/MIGRATION_GUIDE.md` - NEW (user-facing migration guide)
- `mkdocs.yml` - Add navigation

**Critical Blockers (Phase 2)**:
- `tests/fixtures/integration_docker.py` - Fix scoping
- `.github/workflows/ci.yml` - Re-enable tests
- `tests/fixtures/docker-compose.integration.yml` - NEW

**Highest Impact Tests (Phase 3)**:
- `tests/services/policy/test_cache.py` - NEW (259 stmts)
- `tests/services/policy/test_opa_client.py` - NEW (216 stmts)
- `tests/config/test_settings.py` - NEW (209 stmts)
- `tests/api/routers/test_auth.py` - NEW (high security risk)
- `tests/services/auth/test_jwt.py` - NEW (critical security)

---

## Success Metrics

| Phase | Metric | Current | Target |
|-------|--------|---------|--------|
| 1 | Migration guide | No | Yes |
| 1 | Version consistency | Inconsistent | 100% |
| 1 | GitHub release | No | Yes |
| 2 | Collection errors | 84 | 0 |
| 2 | CI tests | Disabled | Enabled |
| 3 | Overall coverage | 6.04% | 85%+ |
| 3 | Zero-coverage modules | 134 | < 20 |
| 3 | High-risk module coverage | 56-79% | 90%+ |
| 4 | Audit documents | 0 | 5 |

---

## Risk Mitigation

**Risk**: Test infrastructure fixes take longer than expected
- **Impact**: Blocks Phase 3
- **Mitigation**: Start with unit tests (no Docker), fix collection errors incrementally

**Risk**: 85% coverage too ambitious
- **Impact**: Extended timeline
- **Mitigation**: Prioritize high-risk modules to 90%+, accept 70-80% for low-risk

**Risk**: Audit timeline pressure
- **Impact**: Need faster delivery
- **Mitigation**: Do Phase 1 + 4 first, test coverage continues in parallel with audit

---

## Estimated Effort

- Phase 1: 1-2 days (version fix, release finalization, migration guide)
- Phase 2: 3-5 days (test infrastructure fixes)
- Phase 3: 10-15 days (test coverage improvement - parallelizable across 5 workstreams)
- Phase 4: 2-3 days (audit preparation documentation)

**Total**: 16-25 days (depends on parallelization)

**Execution Plan** (based on user preferences):
1. **Standard timeline** - execute in order (Phase 1 → 2 → 3 → 4)
2. **Keep 85% coverage target** - comprehensive test improvement
3. **Focus on Migration Guide** - streamlined Phase 1 documentation

**Recommended First Steps**:
1. Execute Phase 1 tasks (version fix, release, migration guide)
2. Fix test infrastructure (Phase 2) - critical blocker
3. Scale test writing (Phase 3) across multiple workstreams in parallel
4. Complete audit documentation (Phase 4)

```

---

## Analysis Framework

### 1. PROJECT OVERVIEW

**Analyze and extract:**

```json
{
  "project_name": "<extracted from plan>",
  "project_type": "<web app|API|library|CLI tool|mobile app|etc>",
  "complexity": "<simple|medium|medium-high|high|very complex>",
  "tech_stack": {
    "backend": ["<technologies>"],
    "frontend": ["<technologies>"],
    "database": ["<technologies>"],
    "infrastructure": ["<technologies>"]
  },
  "estimated_total_tokens": "<sum of all features>"
}
```

**Complexity factors:**
- Simple: Single component, < 500K tokens
- Medium: 2-3 components, 500K-1M tokens
- Medium-High: 3-5 components, 1M-2M tokens
- High: 5-10 components, 2M-4M tokens
- Very Complex: 10+ components, > 4M tokens

---

### 2. FEATURE BREAKDOWN

For each major feature in the plan:

```json
{
  "feature": "<feature name>",
  "description": "<brief description>",
  "complexity": "<simple|medium|high>",
  "tokens_estimated": <number>,
  "dependencies": ["<other features this depends on>"],
  "workers_suggested": ["<worker IDs>"],
  "version_suggested": "<v0.X.Y or v0.X.Y-phaseN>",
  "completion_criteria": ["<specific deliverables>"]
}
```

**Token estimation guidelines:**
- Simple feature: 10K-30K tokens
  - Single file changes
  - Configuration updates
  - Simple bug fixes

- Medium feature: 30K-100K tokens
  - New component
  - API endpoint with tests
  - UI screen with state

- High feature: 100K-300K tokens
  - Architecture changes
  - Multi-component integration
  - Complex algorithms

- Very high feature: 300K-500K tokens (requires phases)
  - Real-time systems
  - Authentication systems
  - Major refactors

**Complexity multipliers:**
- Testing: 1.2x (unit + integration tests)
- Documentation: 1.1x (API docs, guides)
- Integration: 1.3x (connecting multiple systems)
- Refactoring: 1.5x-3x (legacy code complexity)
- Real-time: 1.4x (WebSocket, streaming)
- Security: 1.3x (authentication, authorization)

---

### 3. VERSION PLANNING

Create versions following semantic versioning + phases:

```json
{
  "version": "v0.X.Y[-phaseN]",
  "description": "<what this version delivers>",
  "features_included": ["<feature names>"],
  "token_budget": {
    "projected": <total tokens for version>
  },
  "workers_assigned": ["<worker IDs>"],
  "dependencies": ["<previous versions required>"],
  "completion_criteria": [
    "<specific deliverable>",
    "<test coverage target>",
    "<documentation requirement>"
  ]
}
```

**Version progression rules:**
- v0.1.0: Foundation and architecture (1 architect, 100K-250K tokens)
- v0.2.0+: Major feature sets (2-4 workers, 200K-500K tokens)
- v0.X.Y-phase1, phase2: Large features split into phases (< 500K tokens each)
- v1.0.0: Production ready (testing, hardening, docs, 200K-400K tokens)

**When to use phases:**
- Feature estimate > 300K tokens → Split into phases
- Multiple integration points → One phase per integration
- Sequential dependencies → Phase for each dependency
- Testing requires stages → Phase for each test stage

**Phase naming:**
```
v0.2.1-phase1   First part of large feature
v0.2.1-phase2   Second part
v0.2.1-phase3   Third part (if needed)
v0.3.0          Integration of all phases
```

---

### 4. WORKER ALLOCATION

Recommend workers based on:

**Worker types by role:**
- **Architect**: System design, high-level architecture
  - Best agent: claude-code (better at big picture)
  - Token budget: 100K-300K per architecture phase

- **Backend Developer**: APIs, services, business logic
  - Best agent: claude-code (comprehensive understanding)
  - Token budget: 150K-400K per version

- **Frontend Developer**: UI components, state management
  - Best agent: claude-code (excellent at UI/UX)
  - Token budget: 120K-350K per version

- **Full-Stack Developer**: Both backend and frontend
  - Best agent: claude-code (handles full stack well)
  - Token budget: 200K-500K per version

- **QA Engineer**: Testing, quality assurance
  - Best agent: claude-code (good at test automation)
  - Token budget: 100K-250K per version

- **DevOps Engineer**: CI/CD, infrastructure
  - Best agent: claude-code (shell scripting & config)
  - Token budget: 80K-200K per version

- **Documentation Writer**: Technical writing
  - Best agent: claude-code (better prose)
  - Token budget: 50K-150K per version

**Worker count guidelines:**
- Simple project (< 500K): 1-2 workers
- Medium project (500K-1M): 2-4 workers
- Medium-High (1M-2M): 4-7 workers
- High (2M-4M): 7-12 workers
- Very Complex (> 4M): 12-20 workers

**For each worker:**
```json
{
  "id": "<descriptive-id>",
  "role": "<role type>",
  "agent": "claude-code",
  "description": "<clear role description>",
  "versions_assigned": ["v0.X.Y", "v0.X.Y-phaseN"],
  "total_token_budget": <sum across versions>,
  "rationale": "<why this worker and agent?>"
}
```

---

### 5. GENERATED WORKER PROMPTS

For each worker, generate a complete prompt file:

```markdown
# {Worker Role Title}

## Role
{Clear description of this worker's role}

## Version Assignments
{List of versions this worker participates in}

## Responsibilities

### v{X.Y.Z}[-phaseN] ({Description} - {Projected Tokens}K tokens)
- {Specific responsibility 1}
- {Specific responsibility 2}
- {Specific responsibility 3}

### v{Next Version} ({Description} - {Projected Tokens}K tokens)
- {Specific responsibility 1}
- {Specific responsibility 2}

## Files
- {Directory or file patterns this worker should focus on}

## Tech Stack
- {Technologies this worker will use}

## Token Budget
Total: {Total}K tokens
- v{X.Y.Z}: {Amount}K tokens
- v{Next}: {Amount}K tokens

## Git Workflow
Branches by version:
- v{X.Y.Z}: feat/v{X.Y.Z}-{worker-id}
- v{Next}: feat/v{Next}-{worker-id}

When complete:
1. Commit changes
2. Push to branch
3. Create PR to main
4. Update token metrics in status

## Pattern Library
Review before starting:
- czarina-core/patterns/ERROR_RECOVERY_PATTERNS.md
- czarina-core/patterns/CZARINA_PATTERNS.md

## Version Completion Criteria

### v{X.Y.Z} Complete When:
- [ ] {Deliverable 1}
- [ ] {Deliverable 2}
- [ ] {Test coverage target}
- [ ] Token budget: ≤ {110% of projected}K

### v{Next} Complete When:
- [ ] {Deliverable 1}
- [ ] {Deliverable 2}
- [ ] {Test coverage target}
- [ ] Token budget: ≤ {110% of projected}K
```

---

## Output Format

Generate complete JSON following this schema:

```json
{
  "analysis": {
    "project_name": "string",
    "project_type": "string",
    "complexity": "simple|medium|medium-high|high|very complex",
    "tech_stack": {
      "backend": ["string"],
      "frontend": ["string"],
      "database": ["string"],
      "infrastructure": ["string"]
    },
    "total_tokens_projected": number,
    "recommended_workers": number,
    "recommended_versions": number,
    "efficiency_factors": {
      "description_of_factor": number (1.0 = baseline, > 1.0 = additional complexity)
    }
  },

  "feature_analysis": [
    {
      "feature": "string",
      "description": "string",
      "complexity": "simple|medium|high",
      "tokens_estimated": number,
      "dependencies": ["string"],
      "workers_suggested": ["string"],
      "version_suggested": "vX.Y.Z[-phaseN]",
      "completion_criteria": ["string"]
    }
  ],

  "version_plan": [
    {
      "version": "vX.Y.Z[-phaseN]",
      "description": "string",
      "features_included": ["string"],
      "token_budget": {
        "projected": number
      },
      "workers_assigned": ["string"],
      "dependencies": ["string"],
      "completion_criteria": ["string"]
    }
  ],

  "worker_recommendations": [
    {
      "id": "string",
      "role": "string",
      "agent": "claude-code",
      "description": "string",
      "versions_assigned": ["vX.Y.Z"],
      "total_token_budget": number,
      "rationale": "string"
    }
  ],

  "generated_prompts": {
    "worker-id": "string (full markdown prompt content)"
  },

  "analysis_metadata": {
    "analyzed_at": "ISO 8601 timestamp",
    "analyzer_version": "string",
    "input_file": "string",
    "analysis_tokens_used": number
  }
}
```

---

## Validation Checklist

Before returning analysis, verify:

- [ ] NO time-based estimates anywhere (weeks, days, sprints)
- [ ] All versions use semantic versioning + phases
- [ ] Phases used for features > 300K tokens
- [ ] Token budgets for all versions and workers
- [ ] Worker count appropriate for project size
- [ ] Agent selection rationale provided
- [ ] Worker prompts are complete and actionable
- [ ] Dependencies between versions identified
- [ ] Completion criteria are specific and measurable
- [ ] Total token budget is reasonable for project complexity

---

## Example Analysis Output

See: czarina-inbox/features/2025-11-30-project-analysis.md

for complete example with all generated artifacts.

---

**Version:** 1.0
**Last Updated:** 2025-11-30
**Usage:** `czarina analyze <plan-file>`


---

IMPORTANT: Return ONLY the JSON output following the schema above. Do not include any markdown code fences, explanatory text, or other content. Start your response with { and end with }.