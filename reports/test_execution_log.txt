============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.1, pluggy-1.6.0 -- /usr/local/bin/python
cachedir: .pytest_cache
rootdir: /home/user/sark
configfile: pyproject.toml
plugins: mock-3.15.1, asyncio-1.3.0, Faker-38.2.0, anyio-4.11.0, cov-7.0.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 681 items / 26 deselected / 655 selected

tests/e2e/test_complete_flows.py::test_complete_user_registration_to_server_management FAILED [  0%]
tests/e2e/test_complete_flows.py::test_admin_policy_creation_to_enforcement FAILED [  0%]
tests/e2e/test_complete_flows.py::test_multi_team_server_sharing FAILED  [  0%]
tests/e2e/test_complete_flows.py::test_complete_audit_trail_flow FAILED  [  0%]
tests/e2e/test_data_generator.py::test_generate_user FAILED              [  0%]
tests/e2e/test_data_generator.py::test_generate_users FAILED             [  0%]
tests/e2e/test_data_generator.py::test_generate_mcp_server FAILED        [  1%]
tests/e2e/test_data_generator.py::test_generate_servers FAILED           [  1%]
tests/e2e/test_data_generator.py::test_generate_server_with_tools FAILED [  1%]
tests/e2e/test_data_generator.py::test_generate_policy FAILED            [  1%]
tests/e2e/test_data_generator.py::test_generate_audit_event FAILED       [  1%]
tests/e2e/test_data_generator.py::test_generate_audit_trail FAILED       [  1%]
tests/e2e/test_data_generator.py::test_generate_realistic_dataset FAILED [  1%]
tests/e2e/test_smoke.py::test_database_connectivity PASSED               [  2%]
tests/e2e/test_smoke.py::test_opa_service_availability PASSED            [  2%]
tests/e2e/test_smoke.py::test_redis_cache_connectivity PASSED            [  2%]
tests/e2e/test_smoke.py::test_consul_service_registry PASSED             [  2%]
tests/e2e/test_smoke.py::test_health_check_endpoint PASSED               [  2%]
tests/e2e/test_smoke.py::test_readiness_check_endpoint PASSED            [  2%]
tests/e2e/test_smoke.py::test_metrics_endpoint PASSED                    [  3%]
tests/e2e/test_smoke.py::test_api_documentation_endpoint PASSED          [  3%]
tests/e2e/test_smoke.py::test_jwt_token_generation PASSED                [  3%]
tests/e2e/test_smoke.py::test_jwt_token_validation PASSED                [  3%]
tests/e2e/test_smoke.py::test_session_creation PASSED                    [  3%]
tests/e2e/test_smoke.py::test_api_key_authentication PASSED              [  3%]
tests/e2e/test_smoke.py::test_server_registration PASSED                 [  3%]
tests/e2e/test_smoke.py::test_server_retrieval PASSED                    [  4%]
tests/e2e/test_smoke.py::test_server_search PASSED                       [  4%]
tests/e2e/test_smoke.py::test_server_status_update PASSED                [  4%]
tests/e2e/test_smoke.py::test_policy_evaluation PASSED                   [  4%]
tests/e2e/test_smoke.py::test_authorization_decision PASSED              [  4%]
tests/e2e/test_smoke.py::test_fail_closed_verification PASSED            [  4%]
tests/e2e/test_smoke.py::test_audit_event_creation PASSED                [  5%]
tests/e2e/test_smoke.py::test_siem_forwarding PASSED                     [  5%]
tests/e2e/test_smoke.py::test_jwt_generation_performance PASSED          [  5%]
tests/e2e/test_smoke.py::test_server_search_performance PASSED           [  5%]
tests/e2e/test_smoke.py::test_end_to_end_quick_flow PASSED               [  5%]
tests/integration/test_api_integration.py::test_register_server_endpoint ERROR [  5%]
tests/integration/test_api_integration.py::test_register_server_requires_authentication PASSED [  5%]
tests/integration/test_api_integration.py::test_register_server_validates_input ERROR [  6%]
tests/integration/test_api_integration.py::test_get_server_by_id ERROR   [  6%]
tests/integration/test_api_integration.py::test_update_server_endpoint ERROR [  6%]
tests/integration/test_api_integration.py::test_delete_server_endpoint ERROR [  6%]
tests/integration/test_api_integration.py::test_search_servers_by_name ERROR [  6%]
tests/integration/test_api_integration.py::test_filter_servers_by_status ERROR [  6%]
tests/integration/test_api_integration.py::test_filter_servers_by_sensitivity ERROR [  7%]
tests/integration/test_api_integration.py::test_pagination_first_page ERROR [  7%]
tests/integration/test_api_integration.py::test_pagination_last_page ERROR [  7%]
tests/integration/test_api_integration.py::test_pagination_empty_results ERROR [  7%]
tests/integration/test_api_integration.py::test_bulk_register_servers_transactional ERROR [  7%]
tests/integration/test_api_integration.py::test_bulk_register_servers_best_effort ERROR [  7%]
tests/integration/test_api_integration.py::test_bulk_operation_rollback_on_error ERROR [  7%]
tests/integration/test_api_integration.py::test_404_for_nonexistent_server ERROR [  8%]
tests/integration/test_api_integration.py::test_validation_error_response ERROR [  8%]
tests/integration/test_api_integration.py::test_unauthorized_access_response PASSED [  8%]
tests/integration/test_api_integration.py::test_forbidden_access_response ERROR [  8%]
tests/integration/test_api_integration.py::test_pagination_performance_consistency ERROR [  8%]
tests/integration/test_api_integration.py::test_pagination_edge_cases ERROR [  8%]
tests/integration/test_api_integration.py::test_search_performance_with_large_dataset ERROR [  9%]
tests/integration/test_api_integration.py::test_multi_filter_performance ERROR [  9%]
tests/integration/test_api_integration.py::test_complex_search_filters ERROR [  9%]
tests/integration/test_api_integration.py::test_bulk_register_best_effort_mode ERROR [  9%]
tests/integration/test_api_integration.py::test_bulk_update_servers ERROR [  9%]
tests/integration/test_api_integration.py::test_bulk_delete_servers ERROR [  9%]
tests/integration/test_api_integration.py::test_authentication_required_for_all_endpoints PASSED [  9%]
tests/integration/test_api_integration.py::test_valid_jwt_token_grants_access PASSED [ 10%]
tests/integration/test_api_integration.py::test_expired_token_rejected PASSED [ 10%]
tests/integration/test_api_integration.py::test_invalid_token_rejected PASSED [ 10%]
tests/integration/test_api_integration.py::test_admin_only_endpoints_require_admin_role ERROR [ 10%]
tests/integration/test_api_integration.py::test_malformed_json_rejected ERROR [ 10%]
tests/integration/test_api_integration.py::test_missing_required_fields ERROR [ 10%]
tests/integration/test_api_integration.py::test_invalid_field_types ERROR [ 10%]
tests/integration/test_api_integration.py::test_field_length_validation ERROR [ 11%]
tests/integration/test_api_integration.py::test_duplicate_name_rejected ERROR [ 11%]
tests/integration/test_api_integration.py::test_invalid_sensitivity_level ERROR [ 11%]
tests/integration/test_api_integration.py::test_sql_injection_attempts_blocked ERROR [ 11%]
tests/integration/test_api_integration.py::test_xss_attempts_sanitized ERROR [ 11%]
tests/integration/test_api_integration.py::test_api_latency_p95_under_100ms ERROR [ 11%]
tests/integration/test_api_integration.py::test_concurrent_request_handling ERROR [ 12%]
tests/integration/test_auth_integration.py::test_complete_login_flow FAILED [ 12%]
tests/integration/test_auth_integration.py::test_token_refresh_flow FAILED [ 12%]
tests/integration/test_auth_integration.py::test_logout_invalidates_session FAILED [ 12%]
tests/integration/test_auth_integration.py::test_invalid_credentials_rejected FAILED [ 12%]
tests/integration/test_auth_integration.py::test_expired_token_rejected FAILED [ 12%]
tests/integration/test_auth_integration.py::test_wrong_token_type_rejected FAILED [ 12%]
tests/integration/test_auth_integration.py::test_malformed_token_rejected FAILED [ 13%]
tests/integration/test_auth_integration.py::test_opa_authorization_allows_valid_request ERROR [ 13%]
tests/integration/test_auth_integration.py::test_opa_authorization_denies_invalid_request ERROR [ 13%]
tests/integration/test_auth_integration.py::test_opa_fail_closed_on_error ERROR [ 13%]
tests/integration/test_auth_integration.py::test_admin_has_elevated_permissions ERROR [ 13%]
tests/integration/test_auth_integration.py::test_regular_user_lacks_admin_permissions ERROR [ 13%]
tests/integration/test_auth_integration.py::test_api_key_authentication_flow PASSED [ 14%]
tests/integration/test_policy_integration.py::test_policy_allows_server_registration ERROR [ 14%]
tests/integration/test_policy_integration.py::test_policy_denies_unauthorized_registration ERROR [ 14%]
tests/integration/test_policy_integration.py::test_policy_enforces_sensitivity_levels ERROR [ 14%]
tests/integration/test_policy_integration.py::test_policy_allows_tool_invocation ERROR [ 14%]
tests/integration/test_policy_integration.py::test_policy_denies_dangerous_tool_invocation ERROR [ 14%]
tests/integration/test_policy_integration.py::test_policy_evaluates_bulk_operations ERROR [ 14%]
tests/integration/test_policy_integration.py::test_policy_fails_bulk_on_single_denial ERROR [ 15%]
tests/integration/test_policy_integration.py::test_policy_denial_provides_reason ERROR [ 15%]
tests/integration/test_policy_integration.py::test_policy_handles_opa_errors_gracefully ERROR [ 15%]
tests/integration/test_policy_integration.py::test_create_multiple_policy_versions ERROR [ 15%]
tests/integration/test_policy_integration.py::test_activate_policy_version ERROR [ 15%]
tests/integration/test_policy_integration.py::test_fail_closed_on_network_error ERROR [ 15%]
tests/integration/test_policy_integration.py::test_fail_closed_on_invalid_response ERROR [ 16%]
tests/integration/test_policy_integration.py::test_fail_closed_on_missing_result ERROR [ 16%]
tests/integration/test_siem_integration.py::test_audit_event_persisted_to_database ERROR [ 16%]
tests/integration/test_siem_integration.py::test_audit_events_include_metadata ERROR [ 16%]
tests/integration/test_siem_integration.py::test_high_severity_events_forwarded_to_splunk ERROR [ 16%]
tests/integration/test_siem_integration.py::test_low_severity_events_not_forwarded_to_siem ERROR [ 16%]
tests/integration/test_siem_integration.py::test_events_routed_by_severity ERROR [ 16%]
tests/integration/test_siem_integration.py::test_events_filtered_by_type ERROR [ 17%]
tests/integration/test_siem_integration.py::test_continues_on_siem_failure ERROR [ 17%]
tests/integration/test_siem_integration.py::test_handles_siem_authentication_failure ERROR [ 17%]
tests/integration/test_siem_integration.py::test_handles_siem_network_timeout ERROR [ 17%]
tests/integration/test_siem_integration.py::test_audit_trail_captures_all_operations ERROR [ 17%]
tests/integration/test_siem_integration.py::test_audit_events_include_correlation_data ERROR [ 17%]
tests/test_api.py::test_health_check PASSED                              [ 18%]
tests/test_api.py::test_readiness_check PASSED                           [ 18%]
tests/test_api.py::test_openapi_schema PASSED                            [ 18%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_default_pagination FAILED [ 18%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_custom_limit FAILED [ 18%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_with_cursor FAILED [ 18%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_ascending_order FAILED [ 18%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_with_total FAILED [ 19%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_invalid_limit_too_low FAILED [ 19%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_invalid_limit_too_high FAILED [ 19%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_invalid_sort_order FAILED [ 19%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_empty_results FAILED [ 19%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_response_schema FAILED [ 19%]
tests/test_api_pagination.py::TestServerListPagination::test_list_servers_multiple_pages FAILED [ 20%]
tests/test_api_pagination.py::TestServerListPagination::test_openapi_schema_includes_pagination FAILED [ 20%]
tests/test_bulk_operations.py::TestBulkOperationResult::test_initial_state PASSED [ 20%]
tests/test_bulk_operations.py::TestBulkOperationResult::test_add_success PASSED [ 20%]
tests/test_bulk_operations.py::TestBulkOperationResult::test_add_failure PASSED [ 20%]
tests/test_bulk_operations.py::TestBulkOperationResult::test_to_dict PASSED [ 20%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_register_best_effort_all_success PASSED [ 20%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_register_best_effort_partial_success PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_register_transactional_all_success PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_register_transactional_policy_denied PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_register_transactional_rollback_on_error PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_update_status_best_effort PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_update_status_best_effort_partial_failure PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_batch_policy_evaluation PASSED [ 21%]
tests/test_bulk_operations.py::TestBulkOperationsService::test_batch_policy_evaluation_error PASSED [ 22%]
tests/test_example.py::test_version PASSED                               [ 22%]
tests/test_example.py::test_sample_fixture PASSED                        [ 22%]
tests/test_models.py::TestMCPServerModel::test_server_creation PASSED    [ 22%]
tests/test_models.py::TestMCPServerModel::test_server_repr PASSED        [ 22%]
tests/test_models.py::TestUserModel::test_user_creation PASSED           [ 22%]
tests/test_models.py::TestAuditEventModel::test_audit_event_creation PASSED [ 23%]
tests/test_pagination.py::TestPaginationParams::test_default_values PASSED [ 23%]
tests/test_pagination.py::TestPaginationParams::test_custom_values PASSED [ 23%]
tests/test_pagination.py::TestPaginationParams::test_limit_validation_min PASSED [ 23%]
tests/test_pagination.py::TestPaginationParams::test_limit_validation_max PASSED [ 23%]
tests/test_pagination.py::TestPaginationParams::test_sort_order_validation PASSED [ 23%]
tests/test_pagination.py::TestPaginatedResponse::test_basic_response PASSED [ 23%]
tests/test_pagination.py::TestPaginatedResponse::test_last_page_response PASSED [ 24%]
tests/test_pagination.py::TestPaginatedResponse::test_optional_total PASSED [ 24%]
tests/test_pagination.py::TestCursorPaginator::test_encode_uuid PASSED   [ 24%]
tests/test_pagination.py::TestCursorPaginator::test_encode_string PASSED [ 24%]
tests/test_pagination.py::TestCursorPaginator::test_encode_int PASSED    [ 24%]
tests/test_pagination.py::TestCursorPaginator::test_decode_cursor PASSED [ 24%]
tests/test_pagination.py::TestCursorPaginator::test_decode_invalid_cursor PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_encode_decode_uuid_roundtrip PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_paginate_first_page PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_paginate_last_page PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_paginate_with_total_count PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_paginate_ascending_order PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_paginate_empty_results PASSED [ 25%]
tests/test_pagination.py::TestCursorPaginator::test_paginate_invalid_cursor_ignored PASSED [ 26%]
tests/test_pagination_with_filters.py::TestPaginationWithFilters::test_pagination_with_status_filter PASSED [ 26%]
tests/test_pagination_with_filters.py::TestPaginationWithFilters::test_pagination_with_multiple_filters PASSED [ 26%]
tests/test_pagination_with_filters.py::TestPaginationWithFilters::test_pagination_with_search PASSED [ 26%]
tests/test_pagination_with_filters.py::TestPaginationWithFilters::test_pagination_with_search_and_filters PASSED [ 26%]
tests/test_pagination_with_filters.py::TestPaginationWithFilters::test_filtered_pagination_multiple_pages PASSED [ 26%]
tests/test_pagination_with_filters.py::TestPaginationWithFilters::test_empty_results_with_filters PASSED [ 27%]
tests/test_policy_service.py::TestOPAClient::test_authorization_input_creation PASSED [ 27%]
tests/test_policy_service.py::TestOPAClient::test_opa_client_fail_closed PASSED [ 27%]
tests/test_policy_service.py::TestPolicyService::test_policy_service_placeholder PASSED [ 27%]
tests/test_search.py::TestServerSearchFilter::test_empty_filter PASSED   [ 27%]
tests/test_search.py::TestServerSearchFilter::test_status_filter_single PASSED [ 27%]
tests/test_search.py::TestServerSearchFilter::test_status_filter_list PASSED [ 27%]
tests/test_search.py::TestServerSearchFilter::test_status_filter_empty_list PASSED [ 28%]
tests/test_search.py::TestServerSearchFilter::test_status_filter_none PASSED [ 28%]
tests/test_search.py::TestServerSearchFilter::test_sensitivity_filter_single PASSED [ 28%]
tests/test_search.py::TestServerSearchFilter::test_sensitivity_filter_list PASSED [ 28%]
tests/test_search.py::TestServerSearchFilter::test_team_filter_uuid PASSED [ 28%]
tests/test_search.py::TestServerSearchFilter::test_team_filter_string PASSED [ 28%]
tests/test_search.py::TestServerSearchFilter::test_team_filter_invalid_string PASSED [ 29%]
tests/test_search.py::TestServerSearchFilter::test_owner_filter_uuid PASSED [ 29%]
tests/test_search.py::TestServerSearchFilter::test_tags_filter_single_string PASSED [ 29%]
tests/test_search.py::TestServerSearchFilter::test_tags_filter_list PASSED [ 29%]
tests/test_search.py::TestServerSearchFilter::test_tags_filter_match_all PASSED [ 29%]
tests/test_search.py::TestServerSearchFilter::test_tags_filter_empty_list PASSED [ 29%]
tests/test_search.py::TestServerSearchFilter::test_search_filter PASSED  [ 29%]
tests/test_search.py::TestServerSearchFilter::test_search_filter_empty PASSED [ 30%]
tests/test_search.py::TestServerSearchFilter::test_search_filter_whitespace PASSED [ 30%]
tests/test_search.py::TestServerSearchFilter::test_combined_filters PASSED [ 30%]
tests/test_search.py::TestServerSearchFilter::test_method_chaining PASSED [ 30%]
tests/test_search.py::TestServerSearchService::test_search_servers_no_filters PASSED [ 30%]
tests/test_search.py::TestServerSearchService::test_search_servers_all_filters PASSED [ 30%]
tests/test_search.py::TestParserFunctions::test_parse_status_filter_valid PASSED [ 30%]
tests/test_search.py::TestParserFunctions::test_parse_status_filter_uppercase PASSED [ 31%]
tests/test_search.py::TestParserFunctions::test_parse_status_filter_mixed_case PASSED [ 31%]
tests/test_search.py::TestParserFunctions::test_parse_status_filter_invalid PASSED [ 31%]
tests/test_search.py::TestParserFunctions::test_parse_status_filter_none PASSED [ 31%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_single PASSED [ 31%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_multiple PASSED [ 31%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_with_spaces PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_with_invalid PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_all_invalid PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_none PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_status_list_empty PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_sensitivity_filter_valid PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_sensitivity_filter_invalid PASSED [ 32%]
tests/test_search.py::TestParserFunctions::test_parse_sensitivity_list_single PASSED [ 33%]
tests/test_search.py::TestParserFunctions::test_parse_sensitivity_list_multiple PASSED [ 33%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_single PASSED [ 33%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_multiple PASSED [ 33%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_with_spaces PASSED [ 33%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_empty PASSED [ 33%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_none PASSED [ 34%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_only_commas PASSED [ 34%]
tests/test_search.py::TestParserFunctions::test_parse_tags_filter_mixed_empty PASSED [ 34%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_minimal PASSED [ 34%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_with_user_info PASSED [ 34%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_with_server_info PASSED [ 34%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_with_tool_info PASSED [ 34%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_with_authorization_decision PASSED [ 35%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_with_network_context PASSED [ 35%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_with_details PASSED [ 35%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_none_details_defaults_to_empty_dict PASSED [ 35%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_all_event_types PASSED [ 35%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogEvent::test_log_event_all_severity_levels PASSED [ 35%]
tests/unit/audit/test_audit_service.py::TestAuditServiceSIEMForwarding::test_high_severity_triggers_siem_forward PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceSIEMForwarding::test_critical_severity_triggers_siem_forward PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceSIEMForwarding::test_low_severity_does_not_trigger_siem PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceSIEMForwarding::test_medium_severity_does_not_trigger_siem PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceSIEMForwarding::test_forward_to_siem_updates_timestamp PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogAuthorizationDecision::test_log_authorization_allow PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogAuthorizationDecision::test_log_authorization_deny PASSED [ 36%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogAuthorizationDecision::test_log_authorization_with_policy_id PASSED [ 37%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogAuthorizationDecision::test_log_authorization_with_server_id PASSED [ 37%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogAuthorizationDecision::test_log_authorization_with_network_context PASSED [ 37%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogAuthorizationDecision::test_log_authorization_with_details PASSED [ 37%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogToolInvocation::test_log_tool_invocation_minimal PASSED [ 37%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogToolInvocation::test_log_tool_invocation_with_parameters PASSED [ 37%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogToolInvocation::test_log_tool_invocation_with_network_context PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogToolInvocation::test_log_tool_invocation_none_parameters PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogServerRegistration::test_log_server_registration_minimal PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogServerRegistration::test_log_server_registration_with_details PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogSecurityViolation::test_log_security_violation_with_user PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogSecurityViolation::test_log_security_violation_anonymous PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogSecurityViolation::test_log_security_violation_with_details PASSED [ 38%]
tests/unit/audit/test_audit_service.py::TestAuditServiceLogSecurityViolation::test_log_security_violation_triggers_siem PASSED [ 39%]
tests/unit/audit/test_audit_service.py::TestAuditServiceErrorScenarios::test_log_event_database_commit_failure PASSED [ 39%]
tests/unit/audit/test_audit_service.py::TestAuditServiceErrorScenarios::test_log_event_database_refresh_failure PASSED [ 39%]
tests/unit/audit/test_audit_service.py::TestAuditServiceErrorScenarios::test_siem_forward_failure_does_not_block_logging PASSED [ 39%]
tests/unit/audit/test_audit_service.py::TestAuditServiceErrorScenarios::test_forward_to_siem_commit_failure PASSED [ 39%]
tests/unit/audit/test_audit_service.py::TestAuditServiceTimestamps::test_log_event_sets_timestamp PASSED [ 39%]
tests/unit/audit/test_audit_service.py::TestAuditServiceTimestamps::test_siem_forward_sets_timestamp PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSIEMForwardingBehavior::test_siem_forward_marks_timestamp PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSIEMForwardingBehavior::test_siem_forward_updates_database PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSIEMForwardingBehavior::test_siem_forward_only_high_and_critical PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSplunkIntegration::test_splunk_http_event_collector_format PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSplunkIntegration::test_splunk_network_timeout_graceful_degradation PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSplunkIntegration::test_splunk_authentication_failure PASSED [ 40%]
tests/unit/audit/test_siem_integration.py::TestSplunkIntegration::test_splunk_rate_limiting PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestDatadogIntegration::test_datadog_log_format PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestDatadogIntegration::test_datadog_api_key_invalid PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestDatadogIntegration::test_datadog_network_unavailable PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestCircuitBreakerPattern::test_circuit_breaker_opens_after_failures PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestCircuitBreakerPattern::test_circuit_breaker_half_open_state PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestCircuitBreakerPattern::test_circuit_breaker_timeout_configuration PASSED [ 41%]
tests/unit/audit/test_siem_integration.py::TestGracefulDegradation::test_audit_succeeds_even_if_siem_fails PASSED [ 42%]
tests/unit/audit/test_siem_integration.py::TestGracefulDegradation::test_fallback_queue_for_failed_siem_events PASSED [ 42%]
tests/unit/audit/test_siem_integration.py::TestGracefulDegradation::test_local_logging_fallback PASSED [ 42%]
tests/unit/audit/test_siem_integration.py::TestMultipleSIEMTargets::test_forward_to_multiple_siems PASSED [ 42%]
tests/unit/audit/test_siem_integration.py::TestMultipleSIEMTargets::test_partial_siem_failure_handling PASSED [ 42%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProviderConfig::test_config_initialization PASSED [ 42%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProviderConfig::test_config_with_bind_credentials PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProviderConfig::test_config_with_custom_search_filter PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProviderConfig::test_config_with_group_search PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_authenticate_user_not_found PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_authenticate_invalid_credentials PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_authenticate_success PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_authenticate_uuid_generation PASSED [ 43%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_authenticate_exception_handling PASSED [ 44%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_validate_token_not_supported PASSED [ 44%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_search_user PASSED [ 44%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_bind_user PASSED [ 44%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_get_user_groups_no_group_base PASSED [ 44%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_get_user_groups_with_group_base PASSED [ 44%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_get_user_info PASSED [ 45%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_health_check_enabled PASSED [ 45%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_health_check_disabled PASSED [ 45%]
tests/unit/auth/providers/test_ldap_provider.py::TestLDAPProvider::test_health_check_no_server_url PASSED [ 45%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProviderConfig::test_config_initialization PASSED [ 45%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProviderConfig::test_config_with_custom_scopes PASSED [ 45%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProviderConfig::test_config_with_redirect_uri PASSED [ 45%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_get_discovery_document PASSED [ 46%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_discovery_document_cached PASSED [ 46%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_authenticate_success PASSED [ 46%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_authenticate_token_exchange_failed PASSED [ 46%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_authenticate_no_id_token PASSED [ 46%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_validate_token_success PASSED [ 46%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_validate_token_failed PASSED [ 47%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_get_userinfo PASSED [ 47%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_get_user_info PASSED [ 47%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_health_check_success PASSED [ 47%]
tests/unit/auth/providers/test_oidc_provider.py::TestOIDCProvider::test_health_check_failed PASSED [ 47%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProviderConfig::test_config_initialization PASSED [ 47%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProviderConfig::test_config_with_slo PASSED [ 47%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProviderConfig::test_config_with_custom_name_id_format PASSED [ 48%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_parse_saml_response_valid PASSED [ 48%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_parse_saml_response_invalid_xml PASSED [ 48%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_parse_saml_response_no_assertion PASSED [ 48%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_extract_user_info PASSED [ 48%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_extract_user_info_single_value_attribute PASSED [ 48%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_authenticate_success PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_authenticate_uuid_generation PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_authenticate_invalid_assertion PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_authenticate_exception_handling PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_validate_token PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_get_user_info PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_generate_authn_request PASSED [ 49%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_generate_authn_request_with_relay_state PASSED [ 50%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_generate_authn_request_unique_ids PASSED [ 50%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_health_check_success PASSED [ 50%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_health_check_disabled PASSED [ 50%]
tests/unit/auth/providers/test_saml_provider.py::TestSAMLProvider::test_health_check_missing_config PASSED [ 50%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_generate_key_returns_string PASSED [ 50%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_generate_key_is_unique PASSED [ 50%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_generate_key_with_custom_length PASSED [ 51%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_generate_key_is_url_safe PASSED [ 51%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_hash_key_returns_string PASSED [ 51%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_hash_key_is_deterministic PASSED [ 51%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_hash_key_different_for_different_inputs PASSED [ 51%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_hash_key_handles_special_characters PASSED [ 51%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_verify_key_valid PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_verify_key_invalid PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_verify_key_case_sensitive PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_basic PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_with_scopes PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_with_description PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_with_expiration PASSED [ 52%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_hash_matches PASSED [ 53%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_unique_ids PASSED [ 53%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_create_api_key_timestamps PASSED [ 53%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_validate_api_key_active PASSED [ 53%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_validate_api_key_inactive PASSED [ 53%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_validate_api_key_expired PASSED [ 53%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_validate_api_key_not_yet_expired PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_validate_api_key_no_expiration PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_scope_present PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_scope_absent PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_scope_wildcard PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_scope_empty_scopes PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_any_scope_single_match PASSED [ 54%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_any_scope_multiple_matches PASSED [ 55%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_any_scope_no_match PASSED [ 55%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_any_scope_wildcard PASSED [ 55%]
tests/unit/auth/test_api_key.py::TestAPIKeyService::test_has_any_scope_empty_check_list PASSED [ 55%]
tests/unit/auth/test_api_key.py::TestGetAPIKey::test_get_api_key_valid PASSED [ 55%]
tests/unit/auth/test_api_key.py::TestGetAPIKey::test_get_api_key_missing PASSED [ 55%]
tests/unit/auth/test_api_key.py::TestGetAPIKey::test_get_api_key_invalid PASSED [ 56%]
tests/unit/auth/test_api_key.py::TestGetAPIKey::test_get_api_key_inactive PASSED [ 56%]
tests/unit/auth/test_api_key.py::TestGetAPIKey::test_get_api_key_expired PASSED [ 56%]
tests/unit/auth/test_api_key.py::TestRequireScope::test_require_scope_has_scope PASSED [ 56%]
tests/unit/auth/test_api_key.py::TestRequireScope::test_require_scope_lacks_scope PASSED [ 56%]
tests/unit/auth/test_api_key.py::TestRequireScope::test_require_scope_wildcard PASSED [ 56%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_create_access_token_basic PASSED [ 56%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_create_access_token_with_teams PASSED [ 57%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_create_access_token_without_teams PASSED [ 57%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_create_access_token_with_extra_claims PASSED [ 57%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_access_token_expiration PASSED [ 57%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_create_refresh_token_basic PASSED [ 57%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_refresh_token_expiration PASSED [ 57%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_valid_access_token PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_expired_token PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_invalid_signature PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_malformed_token PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_refresh_token_as_access_fails PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_token_missing_type_field PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_valid_refresh_token PASSED [ 58%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_expired_refresh_token PASSED [ 59%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_decode_access_token_as_refresh_fails PASSED [ 59%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_handler_initialization_with_defaults PASSED [ 59%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_handler_initialization_with_custom_values PASSED [ 59%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_token_with_uuid_user_id PASSED [ 59%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_token_with_empty_teams_list PASSED [ 59%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_token_with_special_characters_in_email PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestJWTHandler::test_token_with_many_teams PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_valid_token PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_invalid_token PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_expired_token PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_missing_required_claims PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_invalid_uuid PASSED [ 60%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_refresh_token_fails PASSED [ 61%]
tests/unit/auth/test_jwt.py::TestGetCurrentUser::test_get_current_user_without_teams PASSED [ 61%]
tests/unit/auth/test_session.py::TestSessionService::test_generate_session_id_returns_string PASSED [ 61%]
tests/unit/auth/test_session.py::TestSessionService::test_generate_session_id_is_unique PASSED [ 61%]
tests/unit/auth/test_session.py::TestSessionService::test_generate_session_id_is_url_safe PASSED [ 61%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_basic PASSED [ 61%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_with_user_agent PASSED [ 61%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_with_ip_address PASSED [ 62%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_with_metadata PASSED [ 62%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_timestamps PASSED [ 62%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_expiration PASSED [ 62%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_custom_lifetime PASSED [ 62%]
tests/unit/auth/test_session.py::TestSessionService::test_create_session_unique_ids PASSED [ 62%]
tests/unit/auth/test_session.py::TestSessionService::test_validate_session_active PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_validate_session_inactive PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_validate_session_expired PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_validate_session_not_expired PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_refresh_session_updates_timestamps PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_refresh_session_extends_lifetime PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_refresh_session_inactive_fails PASSED [ 63%]
tests/unit/auth/test_session.py::TestSessionService::test_refresh_session_expired_fails PASSED [ 64%]
tests/unit/auth/test_session.py::TestSessionService::test_invalidate_session PASSED [ 64%]
tests/unit/auth/test_session.py::TestSessionService::test_invalidate_session_returns_same_session PASSED [ 64%]
tests/unit/auth/test_session.py::TestSessionService::test_get_remaining_time PASSED [ 64%]
tests/unit/auth/test_session.py::TestSessionService::test_get_remaining_time_negative_when_expired PASSED [ 64%]
tests/unit/auth/test_session.py::TestSessionService::test_is_expired_false_for_active PASSED [ 64%]
tests/unit/auth/test_session.py::TestSessionService::test_is_expired_true_for_expired PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionService::test_update_metadata_merge PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionService::test_update_metadata_replace PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionService::test_update_metadata_overwrite_key PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionStore::test_save_and_get_session PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionStore::test_get_nonexistent_session PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionStore::test_save_overwrites_existing PASSED [ 65%]
tests/unit/auth/test_session.py::TestSessionStore::test_delete_existing_session PASSED [ 66%]
tests/unit/auth/test_session.py::TestSessionStore::test_delete_nonexistent_session PASSED [ 66%]
tests/unit/auth/test_session.py::TestSessionStore::test_get_user_sessions PASSED [ 66%]
tests/unit/auth/test_session.py::TestSessionStore::test_get_user_sessions_empty PASSED [ 66%]
tests/unit/auth/test_session.py::TestSessionStore::test_get_user_sessions_different_users PASSED [ 66%]
tests/unit/auth/test_session.py::TestSessionStore::test_delete_user_sessions PASSED [ 66%]
tests/unit/auth/test_session.py::TestSessionStore::test_delete_user_sessions_preserves_other_users PASSED [ 67%]
tests/unit/auth/test_session.py::TestSessionStore::test_delete_user_sessions_nonexistent_user PASSED [ 67%]
tests/unit/auth/test_session.py::TestSessionStore::test_cleanup_expired_removes_expired PASSED [ 67%]
tests/unit/auth/test_session.py::TestSessionStore::test_cleanup_expired_preserves_active PASSED [ 67%]
tests/unit/auth/test_session.py::TestSessionStore::test_cleanup_expired_mixed PASSED [ 67%]
tests/unit/auth/test_session.py::TestSessionStore::test_cleanup_expired_empty_store PASSED [ 67%]
tests/unit/auth/test_session.py::TestGetSessionStore::test_get_session_store_returns_instance PASSED [ 67%]
tests/unit/auth/test_session.py::TestGetSessionStore::test_get_session_store_returns_singleton PASSED [ 68%]
tests/unit/auth/test_user_context.py::TestUserContext::test_user_context_initialization PASSED [ 68%]
tests/unit/auth/test_user_context.py::TestUserContext::test_user_context_defaults PASSED [ 68%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_role_matching PASSED [ 68%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_role_not_matching PASSED [ 68%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_role_admin_override PASSED [ 68%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_role_case_sensitive PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_in_team_member PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_in_team_not_member PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_in_team_empty_teams PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_in_team_case_sensitive PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_any_team_single_match PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_any_team_multiple_matches PASSED [ 69%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_any_team_no_match PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_any_team_empty_user_teams PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestUserContext::test_has_any_team_empty_check_list PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestUserContext::test_to_dict_complete PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestUserContext::test_to_dict_uuid_conversion PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestUserContext::test_to_dict_with_admin PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_basic PASSED [ 70%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_without_teams PASSED [ 71%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_none_teams PASSED [ 71%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_admin_role PASSED [ 71%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_non_admin_role PASSED [ 71%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_multiple_teams PASSED [ 71%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_always_authenticated PASSED [ 71%]
tests/unit/auth/test_user_context.py::TestExtractUserContext::test_extract_user_context_special_email PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkOperationResult::test_initialization PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkOperationResult::test_add_success PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkOperationResult::test_add_failure PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkOperationResult::test_success_count_property PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkOperationResult::test_failure_count_property PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkOperationResult::test_to_dict PASSED [ 72%]
tests/unit/discovery/test_bulk.py::TestBulkRegisterServers::test_bulk_register_best_effort_all_success PASSED [ 73%]
tests/unit/discovery/test_bulk.py::TestBulkRegisterServers::test_bulk_register_best_effort_partial_failure PASSED [ 73%]
tests/unit/discovery/test_bulk.py::TestBulkRegisterServers::test_bulk_register_best_effort_registration_error PASSED [ 73%]
tests/unit/discovery/test_bulk.py::TestBulkRegisterServers::test_bulk_register_transactional_all_success PASSED [ 73%]
tests/unit/discovery/test_bulk.py::TestBulkRegisterServers::test_bulk_register_transactional_policy_denied PASSED [ 73%]
tests/unit/discovery/test_bulk.py::TestBulkRegisterServers::test_bulk_register_transactional_registration_error PASSED [ 73%]
tests/unit/discovery/test_bulk.py::TestBulkUpdateServerStatus::test_bulk_update_status_best_effort_all_success PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBulkUpdateServerStatus::test_bulk_update_status_best_effort_partial_failure PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBulkUpdateServerStatus::test_bulk_update_status_transactional_all_success PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBulkUpdateServerStatus::test_bulk_update_status_transactional_error PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBatchEvaluatePolicies::test_batch_evaluate_policies_all_allowed PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBatchEvaluatePolicies::test_batch_evaluate_policies_some_denied PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBatchEvaluatePolicies::test_batch_evaluate_policies_evaluation_error PASSED [ 74%]
tests/unit/discovery/test_bulk.py::TestBatchEvaluatePolicies::test_batch_evaluate_policies_includes_context PASSED [ 75%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceRegisterServer::test_register_server_http_success PASSED [ 75%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceRegisterServer::test_register_server_stdio_success PASSED [ 75%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceRegisterServer::test_register_server_with_multiple_tools PASSED [ 75%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceRegisterServer::test_register_server_consul_failure_logs_error PASSED [ 75%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceUpdateServerStatus::test_update_status_success PASSED [ 75%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceUpdateServerStatus::test_update_status_server_not_found PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceUpdateServerStatus::test_update_status_to_unhealthy PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceGetServer::test_get_server_found PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceGetServer::test_get_server_not_found PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceGetServerByName::test_get_server_by_name_found PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceGetServerByName::test_get_server_by_name_not_found PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServers::test_list_servers_no_filters PASSED [ 76%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServers::test_list_servers_with_status_filter PASSED [ 77%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServers::test_list_servers_with_owner_filter PASSED [ 77%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServers::test_list_servers_with_team_filter PASSED [ 77%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServers::test_list_servers_with_multiple_filters PASSED [ 77%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServersPaginated::test_list_servers_paginated_basic PASSED [ 77%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceListServersPaginated::test_list_servers_paginated_with_filters PASSED [ 77%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceGetServerTools::test_get_server_tools_success PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceGetServerTools::test_get_server_tools_no_tools PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceDeregisterServer::test_deregister_server_success PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceDeregisterServer::test_deregister_server_not_found PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceDeregisterServer::test_deregister_server_without_consul_id PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceDeregisterServer::test_deregister_server_consul_failure_continues PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceConsulRegistration::test_register_consul_http_with_health_check PASSED [ 78%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceConsulRegistration::test_register_consul_stdio_no_health_check PASSED [ 79%]
tests/unit/discovery/test_discovery_service.py::TestDiscoveryServiceConsulRegistration::test_register_consul_with_tags PASSED [ 79%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_filter_initialization PASSED [ 79%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_status_single PASSED [ 79%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_status_list PASSED [ 79%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_status_none PASSED [ 79%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_status_empty_list PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_sensitivity_single PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_sensitivity_list PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_sensitivity_none PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_team_uuid PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_team_string_uuid PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_team_invalid_uuid PASSED [ 80%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_team_none PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_owner_uuid PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_owner_string_uuid PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_owner_invalid_uuid PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_tags_single_string PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_tags_list_match_any PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_tags_list_match_all PASSED [ 81%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_tags_none PASSED [ 82%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_tags_empty_list PASSED [ 82%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_search_query PASSED [ 82%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_search_empty_string PASSED [ 82%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_search_whitespace_only PASSED [ 82%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_with_search_none PASSED [ 82%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_build_no_filters PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_build_with_filters PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchFilter::test_method_chaining PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_service_initialization PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_no_filters PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_status PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_multiple_statuses PASSED [ 83%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_sensitivity PASSED [ 84%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_team_and_owner PASSED [ 84%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_tags_any PASSED [ 84%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_tags_all PASSED [ 84%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_search_query PASSED [ 84%]
tests/unit/discovery/test_search.py::TestServerSearchService::test_search_servers_with_all_filters PASSED [ 84%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_valid_status PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_registered_status PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_uppercase_status PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_mixed_case_status PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_invalid_status PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_none_status PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusFilter::test_parse_empty_string PASSED [ 85%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_single_status PASSED [ 86%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_multiple_statuses PASSED [ 86%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_statuses_with_whitespace PASSED [ 86%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_statuses_with_invalid PASSED [ 86%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_all_invalid_statuses PASSED [ 86%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_none_list PASSED [ 86%]
tests/unit/discovery/test_search.py::TestParseStatusList::test_parse_empty_string_list PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityFilter::test_parse_valid_sensitivity PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityFilter::test_parse_critical_sensitivity PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityFilter::test_parse_uppercase_sensitivity PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityFilter::test_parse_mixed_case_sensitivity PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityFilter::test_parse_invalid_sensitivity PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityFilter::test_parse_none_sensitivity PASSED [ 87%]
tests/unit/discovery/test_search.py::TestParseSensitivityList::test_parse_single_sensitivity PASSED [ 88%]
tests/unit/discovery/test_search.py::TestParseSensitivityList::test_parse_multiple_sensitivities PASSED [ 88%]
tests/unit/discovery/test_search.py::TestParseSensitivityList::test_parse_sensitivities_with_whitespace PASSED [ 88%]
tests/unit/discovery/test_search.py::TestParseSensitivityList::test_parse_sensitivities_with_invalid PASSED [ 88%]
tests/unit/discovery/test_search.py::TestParseSensitivityList::test_parse_all_invalid_sensitivities PASSED [ 88%]
tests/unit/discovery/test_search.py::TestParseSensitivityList::test_parse_none_sensitivity_list PASSED [ 88%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_single_tag PASSED [ 89%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_multiple_tags PASSED [ 89%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_tags_with_whitespace PASSED [ 89%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_tags_with_empty_values PASSED [ 89%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_none_tags PASSED [ 89%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_empty_string_tags PASSED [ 89%]
tests/unit/discovery/test_search.py::TestParseTagsFilter::test_parse_all_empty_tags PASSED [ 89%]
tests/unit/discovery/test_servers.py::TestServerRegistration::test_register_http_server_minimal_config PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerRegistration::test_register_server_with_full_config PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerRegistration::test_register_stdio_server PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerRegistration::test_register_sse_server PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerRegistration::test_register_server_with_multiple_capabilities PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerRegistration::test_register_server_with_complex_tools PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerStatusManagement::test_update_status_to_active PASSED [ 90%]
tests/unit/discovery/test_servers.py::TestServerStatusManagement::test_update_status_to_inactive PASSED [ 91%]
tests/unit/discovery/test_servers.py::TestServerStatusManagement::test_update_status_to_unhealthy PASSED [ 91%]
tests/unit/discovery/test_servers.py::TestServerStatusManagement::test_update_status_lifecycle PASSED [ 91%]
tests/unit/discovery/test_servers.py::TestServerRetrieval::test_get_server_by_id PASSED [ 91%]
tests/unit/discovery/test_servers.py::TestServerRetrieval::test_get_server_by_name PASSED [ 91%]
tests/unit/discovery/test_servers.py::TestServerRetrieval::test_get_nonexistent_server_by_id PASSED [ 91%]
tests/unit/discovery/test_servers.py::TestServerRetrieval::test_get_nonexistent_server_by_name PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerTools::test_get_server_tools PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerTools::test_get_tools_for_server_with_no_tools PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerDeregistration::test_deregister_server PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerDeregistration::test_deregister_server_without_consul PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerDeregistration::test_deregister_nonexistent_server PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerSensitivityLevels::test_register_low_sensitivity_server PASSED [ 92%]
tests/unit/discovery/test_servers.py::TestServerSensitivityLevels::test_register_critical_sensitivity_server PASSED [ 93%]
tests/unit/discovery/test_servers.py::TestServerSensitivityLevels::test_register_server_default_sensitivity PASSED [ 93%]
tests/unit/policy/test_opa_client.py::TestAuthorizationInput::test_authorization_input_creation PASSED [ 93%]
tests/unit/policy/test_opa_client.py::TestAuthorizationInput::test_authorization_input_minimal PASSED [ 93%]
tests/unit/policy/test_opa_client.py::TestAuthorizationInput::test_authorization_input_serialization PASSED [ 93%]
tests/unit/policy/test_opa_client.py::TestAuthorizationDecision::test_authorization_decision_allow PASSED [ 93%]
tests/unit/policy/test_opa_client.py::TestAuthorizationDecision::test_authorization_decision_deny PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_client_initialization PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_client_initialization_with_defaults PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_evaluate_policy_allow PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_evaluate_policy_deny PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_evaluate_policy_http_error PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_evaluate_policy_unexpected_error PASSED [ 94%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_evaluate_policy_empty_result PASSED [ 95%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_evaluate_policy_calls_correct_endpoint PASSED [ 95%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_check_tool_access_allow PASSED [ 95%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_check_tool_access_without_teams PASSED [ 95%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_check_tool_access_builds_correct_input PASSED [ 95%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_check_server_registration_allow PASSED [ 95%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_check_server_registration_builds_correct_input PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_authorize_true PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_authorize_false PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_authorize_with_context PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_health_check_success PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_health_check_failure_non_200 PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_health_check_failure_exception PASSED [ 96%]
tests/unit/policy/test_opa_client.py::TestOPAClient::test_close PASSED   [ 97%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_policy_basic PASSED [ 97%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_policy_sets_status_to_draft PASSED [ 97%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_policy_creates_initial_version PASSED [ 97%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_version_increments_version_number PASSED [ 97%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_version_first_version PASSED [ 97%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_version_with_notes PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_create_version_not_active_by_default PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_activate_version_success PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_activate_version_not_found PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_activate_version_policy_not_found PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_get_policy_found PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_get_policy_not_found PASSED [ 98%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_get_policy_by_name_found PASSED [ 99%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_get_policy_by_name_not_found PASSED [ 99%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_list_policies_all PASSED [ 99%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_list_policies_filter_by_type PASSED [ 99%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_list_policies_filter_by_status PASSED [ 99%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_list_policies_filter_by_type_and_status PASSED [ 99%]
tests/unit/policy/test_policy_service.py::TestPolicyService::test_list_policies_empty PASSED [100%]

==================================== ERRORS ====================================
_______________ ERROR at setup of test_register_server_endpoint ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e865de10>
test_user = <User(id=b9619b02-ff53-48d5-bf15-20bb4ce6b048, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
____________ ERROR at setup of test_register_server_validates_input ____________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e86607d0>
test_user = <User(id=17551000-3fe6-47b7-bcc9-cd44f733d938, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
___________________ ERROR at setup of test_get_server_by_id ____________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8647a10>
test_user = <User(id=8627325d-56c1-4fc8-bb33-e3249dcf2f2b, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_update_server_endpoint _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8745850>
test_user = <User(id=bacc03f1-c06e-43e4-90cf-79280d5e74ad, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_delete_server_endpoint _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8617fd0>
admin_user = <User(id=2833c0c3-fab4-4578-b718-bb43b4935e6d, email=admin@example.com, role=admin)>

    @pytest.fixture
    def admin_headers(jwt_handler, admin_user):
        """Generate admin authentication headers."""
>       token = jwt_handler.create_access_token(user_id=admin_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:84: TypeError
________________ ERROR at setup of test_search_servers_by_name _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8cfa550>
test_user = <User(id=485407d1-c705-489b-8671-ff1f9de2bb0d, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_______________ ERROR at setup of test_filter_servers_by_status ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e865e290>
test_user = <User(id=5707d2e7-8607-4924-88d9-8b6b2e58546f, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_____________ ERROR at setup of test_filter_servers_by_sensitivity _____________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e86a6e10>
test_user = <User(id=b4af6db0-494f-4a07-990f-382ea54d9bf5, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_________________ ERROR at setup of test_pagination_first_page _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8606310>
test_user = <User(id=0d0bf754-b94d-419a-a9a0-8517178992e8, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_________________ ERROR at setup of test_pagination_last_page __________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e86a58d0>
test_user = <User(id=594a2666-a725-4652-a8cc-70a99a52e491, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_______________ ERROR at setup of test_pagination_empty_results ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e871ba10>
test_user = <User(id=465f9cf5-324b-4d46-ba2d-629cfebae4ca, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
__________ ERROR at setup of test_bulk_register_servers_transactional __________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8cefe90>
test_user = <User(id=5bda08c0-68b9-400b-a4ee-4e2f3a57d809, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
___________ ERROR at setup of test_bulk_register_servers_best_effort ___________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8712950>
test_user = <User(id=6301267a-0fae-44cd-af34-6290ca200582, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
___________ ERROR at setup of test_bulk_operation_rollback_on_error ____________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87c5f50>
test_user = <User(id=05fbecf5-3a9a-4ce3-bb5b-b0f759e9a18d, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
______________ ERROR at setup of test_404_for_nonexistent_server _______________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8a64190>
test_user = <User(id=d6f04a88-201a-40bb-b0ad-8dd9475d257c, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_______________ ERROR at setup of test_validation_error_response _______________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87cae10>
test_user = <User(id=17ad6870-0b22-424c-a307-5eeea9ed6db3, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_______________ ERROR at setup of test_forbidden_access_response _______________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8b08910>
test_user = <User(id=b1f98344-d961-431b-b12b-b6c91503990e, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
__________ ERROR at setup of test_pagination_performance_consistency ___________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e872add0>
test_user = <User(id=c7fdd86f-8363-4b91-a90a-4ad20414de30, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_________________ ERROR at setup of test_pagination_edge_cases _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8708510>
test_user = <User(id=0784ac63-3cf4-4741-bb6c-8bd22b1b2458, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_________ ERROR at setup of test_search_performance_with_large_dataset _________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87c76d0>
test_user = <User(id=1b075ca7-a675-4afd-88b3-feb1f78f57e3, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_______________ ERROR at setup of test_multi_filter_performance ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e871f410>
test_user = <User(id=5ceb08f1-5739-4863-88d9-b1a2d2a28b58, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_complex_search_filters _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87e1050>
test_user = <User(id=92eb46f5-a1d7-4f25-a7af-8b0f75abcfa7, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
____________ ERROR at setup of test_bulk_register_best_effort_mode _____________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8700850>
test_user = <User(id=4d03c538-a965-4d4c-8de1-fd5722e6aaf8, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
__________________ ERROR at setup of test_bulk_update_servers __________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8710c90>
test_user = <User(id=28e28f57-09d8-4cf0-b6ad-6ac7e11e0efe, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
__________________ ERROR at setup of test_bulk_delete_servers __________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8741710>
admin_user = <User(id=691c9326-0a9e-470e-b4b3-f9e9cf6836c9, email=admin@example.com, role=admin)>

    @pytest.fixture
    def admin_headers(jwt_handler, admin_user):
        """Generate admin authentication headers."""
>       token = jwt_handler.create_access_token(user_id=admin_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:84: TypeError
________ ERROR at setup of test_admin_only_endpoints_require_admin_role ________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e86a7b50>
admin_user = <User(id=9673057f-a7c9-4ff7-95af-99cf5fdf733c, email=admin@example.com, role=admin)>

    @pytest.fixture
    def admin_headers(jwt_handler, admin_user):
        """Generate admin authentication headers."""
>       token = jwt_handler.create_access_token(user_id=admin_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:84: TypeError
________________ ERROR at setup of test_malformed_json_rejected ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e86a5810>
test_user = <User(id=778c7f06-27f0-496e-b75e-a9df75541630, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_missing_required_fields ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87243d0>
test_user = <User(id=2c097bce-231c-4d8e-badb-ac03b67bdc98, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
__________________ ERROR at setup of test_invalid_field_types __________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e864d150>
test_user = <User(id=e16e8e65-0ede-4390-938c-22b8a857f8e9, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_field_length_validation ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87ca810>
test_user = <User(id=5b34ccaa-83c1-42f2-bf0e-aa6adcdad6da, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_duplicate_name_rejected ________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8605890>
test_user = <User(id=3df73c39-bd5e-4f28-a96e-ea7620200b4a, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
_______________ ERROR at setup of test_invalid_sensitivity_level _______________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8617250>
test_user = <User(id=12be2b4e-0b47-4a3a-bf98-2a4d5ae9dfbf, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
____________ ERROR at setup of test_sql_injection_attempts_blocked _____________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e861bb10>
test_user = <User(id=905389f0-b91b-414e-8620-ef738a98e32a, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________________ ERROR at setup of test_xss_attempts_sanitized _________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e861f490>
test_user = <User(id=c15edd0e-5cc4-4ab0-b566-193a66ea25fc, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
______________ ERROR at setup of test_api_latency_p95_under_100ms ______________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8685350>
test_user = <User(id=6ab4773d-d632-4331-b203-f91532d9e017, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
______________ ERROR at setup of test_concurrent_request_handling ______________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e870c850>
test_user = <User(id=d3750fa6-2676-4e84-b7e2-96b17dd81fba, email=test@example.com, role=developer)>

    @pytest.fixture
    def auth_headers(jwt_handler, test_user):
        """Generate authentication headers."""
>       token = jwt_handler.create_access_token(user_id=test_user.id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_api_integration.py:77: TypeError
________ ERROR at setup of test_opa_authorization_allows_valid_request _________

self = <Coroutine test_opa_authorization_allows_valid_request>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_auth_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_opa_authorization_allows_valid_request>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_opa_authorization_allows_valid_request' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_______ ERROR at setup of test_opa_authorization_denies_invalid_request ________

self = <Coroutine test_opa_authorization_denies_invalid_request>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_auth_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_opa_authorization_denies_invalid_request>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_opa_authorization_denies_invalid_request' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_______________ ERROR at setup of test_opa_fail_closed_on_error ________________

self = <Coroutine test_opa_fail_closed_on_error>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_auth_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_opa_fail_closed_on_error>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_opa_fail_closed_on_error' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
____________ ERROR at setup of test_admin_has_elevated_permissions _____________

self = <Coroutine test_admin_has_elevated_permissions>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_auth_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_admin_has_elevated_permissions>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_admin_has_elevated_permissions' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_________ ERROR at setup of test_regular_user_lacks_admin_permissions __________

self = <Coroutine test_regular_user_lacks_admin_permissions>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_auth_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_regular_user_lacks_admin_permissions>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_regular_user_lacks_admin_permissions' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
___________ ERROR at setup of test_policy_allows_server_registration ___________

self = <Coroutine test_policy_allows_server_registration>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_allows_server_registration>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_allows_server_registration' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
________ ERROR at setup of test_policy_denies_unauthorized_registration ________

self = <Coroutine test_policy_denies_unauthorized_registration>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_denies_unauthorized_registration>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_denies_unauthorized_registration' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
__________ ERROR at setup of test_policy_enforces_sensitivity_levels ___________

self = <Coroutine test_policy_enforces_sensitivity_levels>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_enforces_sensitivity_levels>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_enforces_sensitivity_levels' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_____________ ERROR at setup of test_policy_allows_tool_invocation _____________

self = <Coroutine test_policy_allows_tool_invocation>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_allows_tool_invocation>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_allows_tool_invocation' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
________ ERROR at setup of test_policy_denies_dangerous_tool_invocation ________

self = <Coroutine test_policy_denies_dangerous_tool_invocation>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_denies_dangerous_tool_invocation>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_denies_dangerous_tool_invocation' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
___________ ERROR at setup of test_policy_evaluates_bulk_operations ____________

self = <Coroutine test_policy_evaluates_bulk_operations>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_evaluates_bulk_operations>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_evaluates_bulk_operations' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
__________ ERROR at setup of test_policy_fails_bulk_on_single_denial ___________

self = <Coroutine test_policy_fails_bulk_on_single_denial>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_fails_bulk_on_single_denial>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_fails_bulk_on_single_denial' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_____________ ERROR at setup of test_policy_denial_provides_reason _____________

self = <Coroutine test_policy_denial_provides_reason>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_denial_provides_reason>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_denial_provides_reason' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_________ ERROR at setup of test_policy_handles_opa_errors_gracefully __________

self = <Coroutine test_policy_handles_opa_errors_gracefully>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_policy_handles_opa_errors_gracefully>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_policy_handles_opa_errors_gracefully' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
____________ ERROR at setup of test_create_multiple_policy_versions ____________

self = <Coroutine test_create_multiple_policy_versions>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_create_multiple_policy_versions>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_create_multiple_policy_versions' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
________________ ERROR at setup of test_activate_policy_version ________________

self = <Coroutine test_activate_policy_version>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_activate_policy_version>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_activate_policy_version' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_____________ ERROR at setup of test_fail_closed_on_network_error ______________

self = <Coroutine test_fail_closed_on_network_error>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_fail_closed_on_network_error>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_fail_closed_on_network_error' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
____________ ERROR at setup of test_fail_closed_on_invalid_response ____________

self = <Coroutine test_fail_closed_on_invalid_response>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_fail_closed_on_invalid_response>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_fail_closed_on_invalid_response' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
_____________ ERROR at setup of test_fail_closed_on_missing_result _____________

self = <Coroutine test_fail_closed_on_missing_result>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fixturedef = <FixtureDef argname='opa_client' scope='function' baseid='tests/integration/test_policy_integration.py'>
request = <SubRequest 'opa_client' for <Coroutine test_fail_closed_on_missing_result>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^
E               pytest.PytestRemovedIn9Warning: 'test_fail_closed_on_missing_result' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
E               See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture

/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:728: PytestRemovedIn9Warning
___________ ERROR at setup of test_audit_event_persisted_to_database ___________

mock_db_session = <MagicMock id='139530211139728'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_____________ ERROR at setup of test_audit_events_include_metadata _____________

mock_db_session = <MagicMock id='139530207233744'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_______ ERROR at setup of test_high_severity_events_forwarded_to_splunk ________

mock_db_session = <MagicMock id='139530207928208'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_______ ERROR at setup of test_low_severity_events_not_forwarded_to_siem _______

mock_db_session = <MagicMock id='139530213498320'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_______________ ERROR at setup of test_events_routed_by_severity _______________

mock_db_session = <MagicMock id='139530206274640'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
________________ ERROR at setup of test_events_filtered_by_type ________________

mock_db_session = <MagicMock id='139530207971856'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_______________ ERROR at setup of test_continues_on_siem_failure _______________

mock_db_session = <MagicMock id='139530206867856'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
__________ ERROR at setup of test_handles_siem_authentication_failure __________

mock_db_session = <MagicMock id='139530206592336'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_____________ ERROR at setup of test_handles_siem_network_timeout ______________

mock_db_session = <MagicMock id='139530207340816'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
__________ ERROR at setup of test_audit_trail_captures_all_operations __________

mock_db_session = <MagicMock id='139530207240592'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
_________ ERROR at setup of test_audit_events_include_correlation_data _________

mock_db_session = <MagicMock id='139530206291280'>

    @pytest.fixture
    def audit_service(mock_db_session):
        """Audit service for tests."""
>       return AuditService(
            db=mock_db_session,
            siem_enabled=True,
            splunk_url="https://splunk.example.com:8088/services/collector/event",
            splunk_token="test-token",
            datadog_url="https://http-intake.logs.datadoghq.com/v1/input",
            datadog_api_key="test-api-key"
        )
E       TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'

tests/integration/test_siem_integration.py:37: TypeError
=================================== FAILURES ===================================
_____________ test_complete_user_registration_to_server_management _____________

    @pytest.mark.e2e
    @pytest.mark.user_flow
    @pytest.mark.critical
    @pytest.mark.asyncio
    async def test_complete_user_registration_to_server_management():
        """
        Complete user journey: Registration  Login  Server Registration  Management.
    
        Flow:
        1. User registers with email/password
        2. Email verification (mocked)
        3. User logs in and receives JWT token
        4. User registers an MCP server
        5. User retrieves server information
        6. User updates server configuration
        7. User searches for their servers
        8. Audit trail verification
        """
        # Step 1: User Registration
        user_data = {
            "email": "newuser@example.com",
            "password": "SecurePassword123!",
            "full_name": "New User"
        }
    
        # Mock user creation
        user_id = uuid4()
        created_user = User(
            id=user_id,
            email=user_data["email"],
            full_name=user_data["full_name"],
            hashed_password="hashed_password",
            role="developer",
>           status=ServerStatus.ACTIVE,
                   ^^^^^^^^^^^^
            is_admin=False,
            extra_metadata={},
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'ServerStatus' is not defined

tests/e2e/test_complete_flows.py:62: NameError
__________________ test_admin_policy_creation_to_enforcement ___________________

    @pytest.mark.e2e
    @pytest.mark.admin_flow
    @pytest.mark.critical
    @pytest.mark.asyncio
    async def test_admin_policy_creation_to_enforcement():
        """
        Complete admin journey: Policy Creation  Activation  Enforcement.
    
        Flow:
        1. Admin user logs in
        2. Admin creates authorization policy
        3. Admin creates policy version with Rego code
        4. Admin activates policy version
        5. Regular user attempts server registration
        6. Policy is evaluated and enforced
        7. Verify policy decision is logged
        """
        # Step 1: Admin Login
        admin_user = User(
            id=uuid4(),
            email="admin@example.com",
            full_name="Admin User",
            hashed_password="hashed_password",
            role="admin",
>           status=ServerStatus.ACTIVE,
                   ^^^^^^^^^^^^
            is_admin=True,
            extra_metadata={},
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'ServerStatus' is not defined

tests/e2e/test_complete_flows.py:207: NameError
________________________ test_multi_team_server_sharing ________________________

    @pytest.mark.e2e
    @pytest.mark.user_flow
    @pytest.mark.asyncio
    async def test_multi_team_server_sharing():
        """
        Multi-team collaboration workflow.
    
        Flow:
        1. Create multiple teams (Engineering, Data Science)
        2. Create users and assign to teams
        3. Each team registers servers
        4. Configure team-scoped access policies
        5. Verify cross-team access restrictions
        6. Test server discovery within team scope
        7. Test server sharing between teams
        """
        # Step 1: Create Teams
        engineering_team = Team(
            id=uuid4(),
            name="Engineering",
            description="Engineering team",
            extra_metadata={},
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
    
        data_science_team = Team(
            id=uuid4(),
            name="Data Science",
            description="Data Science team",
            extra_metadata={},
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
    
        # Step 2: Create Users and Assign to Teams
        engineer_user = User(
            id=uuid4(),
            email="engineer@example.com",
            full_name="Engineer User",
            hashed_password="hashed_password",
            role="developer",
>           status=ServerStatus.ACTIVE,
                   ^^^^^^^^^^^^
            is_admin=False,
            extra_metadata={"team_id": str(engineering_team.id)},
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'ServerStatus' is not defined

tests/e2e/test_complete_flows.py:429: NameError
________________________ test_complete_audit_trail_flow ________________________

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_complete_audit_trail_flow():
        """
        Complete audit trail workflow.
    
        Flow:
        1. User performs series of actions
        2. Each action generates audit event
        3. High-severity events forwarded to SIEM
        4. Query audit trail for user
        5. Verify event ordering and completeness
        6. Test audit event correlation
        """
        user = User(
            id=uuid4(),
            email="audited@example.com",
            full_name="Audited User",
            hashed_password="hashed_password",
            role="developer",
>           status=ServerStatus.ACTIVE,
                   ^^^^^^^^^^^^
            is_admin=False,
            extra_metadata={},
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'ServerStatus' is not defined

tests/e2e/test_complete_flows.py:570: NameError
______________________________ test_generate_user ______________________________

    def test_generate_user():
        """Test user generation."""
>       user = generate_user()
               ^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:578: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

role = 'developer', is_admin = False, status = <ServerStatus.ACTIVE: 'active'>
team_id = None

    def generate_user(
        role: str = "developer",
        is_admin: bool = False,
        status: ServerStatus = ServerStatus.ACTIVE,
        team_id: Optional[uuid4] = None
    ) -> User:
        """
        Generate a single user with realistic data.
    
        Args:
            role: User role (developer, analyst, admin)
            is_admin: Whether user has admin privileges
            is_active: Whether user account is active
            team_id: Optional team ID for user metadata
    
        Returns:
            User object with realistic data
        """
        first_name = fake.first_name()
        last_name = fake.last_name()
        email = f"{first_name.lower()}.{last_name.lower()}@{fake.domain_name()}"
    
        extra_metadata = {}
        if team_id:
            extra_metadata["team_id"] = str(team_id)
    
        return User(
            id=uuid4(),
            email=email,
            full_name=f"{first_name} {last_name}",
            hashed_password=fake.sha256(),
            role=role,
>           is_active=is_active,
                      ^^^^^^^^^
            is_admin=is_admin,
            extra_metadata=extra_metadata,
            created_at=datetime.now(UTC) - timedelta(days=random.randint(1, 365)),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'is_active' is not defined

tests/e2e/test_data_generator.py:61: NameError
_____________________________ test_generate_users ______________________________

    def test_generate_users():
        """Test multiple user generation."""
>       users = generate_users(10)
                ^^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:587: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/test_data_generator.py:80: in generate_users
    return [generate_user(**kwargs) for _ in range(count)]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/e2e/test_data_generator.py:80: in <listcomp>
    return [generate_user(**kwargs) for _ in range(count)]
            ^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

role = 'developer', is_admin = False, status = <ServerStatus.ACTIVE: 'active'>
team_id = None

    def generate_user(
        role: str = "developer",
        is_admin: bool = False,
        status: ServerStatus = ServerStatus.ACTIVE,
        team_id: Optional[uuid4] = None
    ) -> User:
        """
        Generate a single user with realistic data.
    
        Args:
            role: User role (developer, analyst, admin)
            is_admin: Whether user has admin privileges
            is_active: Whether user account is active
            team_id: Optional team ID for user metadata
    
        Returns:
            User object with realistic data
        """
        first_name = fake.first_name()
        last_name = fake.last_name()
        email = f"{first_name.lower()}.{last_name.lower()}@{fake.domain_name()}"
    
        extra_metadata = {}
        if team_id:
            extra_metadata["team_id"] = str(team_id)
    
        return User(
            id=uuid4(),
            email=email,
            full_name=f"{first_name} {last_name}",
            hashed_password=fake.sha256(),
            role=role,
>           is_active=is_active,
                      ^^^^^^^^^
            is_admin=is_admin,
            extra_metadata=extra_metadata,
            created_at=datetime.now(UTC) - timedelta(days=random.randint(1, 365)),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'is_active' is not defined

tests/e2e/test_data_generator.py:61: NameError
___________________________ test_generate_mcp_server ___________________________

    def test_generate_mcp_server():
        """Test server generation."""
>       server = generate_mcp_server()
                 ^^^^^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:598: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

owner_id = UUID('3e32a0d1-90af-490c-a643-bac61b902a9c')
team_id = UUID('9d286836-cbce-49b7-a98a-e2fdf758b37e')
sensitivity_level = <SensitivityLevel.MEDIUM: 'medium'>
server_status = <ServerStatus.ACTIVE: 'active'>
transport = <TransportType.HTTP: 'http'>

    def generate_mcp_server(
        owner_id: Optional[uuid4] = None,
        team_id: Optional[uuid4] = None,
        sensitivity_level: SensitivityLevel = SensitivityLevel.MEDIUM,
        server_status: ServerStatus = ServerStatus.ACTIVE,
        transport: TransportType = TransportType.HTTP
    ) -> MCPServer:
        """
        Generate a single MCP server with realistic data.
    
        Args:
            owner_id: User ID who owns the server
            team_id: Team ID the server belongs to
            sensitivity_level: Security sensitivity level
            is_active: Whether server is active
            transport: Transport protocol
    
        Returns:
            MCPServer object
        """
        server_types = [
            "api-gateway", "data-processor", "ml-model", "analytics",
            "monitoring", "logging", "auth-service", "notification"
        ]
    
        server_type = random.choice(server_types)
        server_name = f"{server_type}-{fake.word()}-{random.randint(1, 999)}"
    
        if not owner_id:
            owner_id = uuid4()
        if not team_id:
            team_id = uuid4()
    
        # Generate endpoint based on transport type
        if transport == TransportType.HTTP:
            endpoint = f"http://{server_name}.{fake.domain_name()}/mcp"
        elif transport == TransportType.STDIO:
            endpoint = f"/usr/local/bin/{server_name}"
        else:  # SSE
            endpoint = f"http://{server_name}.{fake.domain_name()}/events"
    
        return MCPServer(
            id=uuid4(),
            name=server_name,
            description=fake.sentence(),
            transport=transport,
            endpoint=endpoint,
            sensitivity_level=sensitivity_level,
            owner_id=owner_id,
            team_id=team_id,
>           is_active=is_active,
                      ^^^^^^^^^
            health_check_url=f"http://{server_name}.{fake.domain_name()}/health" if transport == TransportType.HTTP else None,
            created_at=datetime.now(UTC) - timedelta(days=random.randint(1, 180)),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'is_active' is not defined

tests/e2e/test_data_generator.py:170: NameError
____________________________ test_generate_servers _____________________________

    def test_generate_servers():
        """Test multiple server generation."""
>       servers = generate_servers(20)
                  ^^^^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/test_data_generator.py:191: in generate_servers
    return [generate_mcp_server(**kwargs) for _ in range(count)]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/e2e/test_data_generator.py:191: in <listcomp>
    return [generate_mcp_server(**kwargs) for _ in range(count)]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

owner_id = UUID('c871e896-ed90-40d6-8731-dc9af770348e')
team_id = UUID('176618db-439f-4a55-983e-03816da001df')
sensitivity_level = <SensitivityLevel.MEDIUM: 'medium'>
server_status = <ServerStatus.ACTIVE: 'active'>
transport = <TransportType.HTTP: 'http'>

    def generate_mcp_server(
        owner_id: Optional[uuid4] = None,
        team_id: Optional[uuid4] = None,
        sensitivity_level: SensitivityLevel = SensitivityLevel.MEDIUM,
        server_status: ServerStatus = ServerStatus.ACTIVE,
        transport: TransportType = TransportType.HTTP
    ) -> MCPServer:
        """
        Generate a single MCP server with realistic data.
    
        Args:
            owner_id: User ID who owns the server
            team_id: Team ID the server belongs to
            sensitivity_level: Security sensitivity level
            is_active: Whether server is active
            transport: Transport protocol
    
        Returns:
            MCPServer object
        """
        server_types = [
            "api-gateway", "data-processor", "ml-model", "analytics",
            "monitoring", "logging", "auth-service", "notification"
        ]
    
        server_type = random.choice(server_types)
        server_name = f"{server_type}-{fake.word()}-{random.randint(1, 999)}"
    
        if not owner_id:
            owner_id = uuid4()
        if not team_id:
            team_id = uuid4()
    
        # Generate endpoint based on transport type
        if transport == TransportType.HTTP:
            endpoint = f"http://{server_name}.{fake.domain_name()}/mcp"
        elif transport == TransportType.STDIO:
            endpoint = f"/usr/local/bin/{server_name}"
        else:  # SSE
            endpoint = f"http://{server_name}.{fake.domain_name()}/events"
    
        return MCPServer(
            id=uuid4(),
            name=server_name,
            description=fake.sentence(),
            transport=transport,
            endpoint=endpoint,
            sensitivity_level=sensitivity_level,
            owner_id=owner_id,
            team_id=team_id,
>           is_active=is_active,
                      ^^^^^^^^^
            health_check_url=f"http://{server_name}.{fake.domain_name()}/health" if transport == TransportType.HTTP else None,
            created_at=datetime.now(UTC) - timedelta(days=random.randint(1, 180)),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'is_active' is not defined

tests/e2e/test_data_generator.py:170: NameError
_______________________ test_generate_server_with_tools ________________________

    def test_generate_server_with_tools():
        """Test server with tools generation."""
>       server = generate_server_with_tools(tool_count=5, prompt_count=3)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:614: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/test_data_generator.py:212: in generate_server_with_tools
    server = generate_mcp_server(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

owner_id = UUID('33784b27-cf75-490f-b093-d9990597b0b5')
team_id = UUID('7dc2b580-10f1-4df8-9216-5fc60c56cf30')
sensitivity_level = <SensitivityLevel.MEDIUM: 'medium'>
server_status = <ServerStatus.ACTIVE: 'active'>
transport = <TransportType.HTTP: 'http'>

    def generate_mcp_server(
        owner_id: Optional[uuid4] = None,
        team_id: Optional[uuid4] = None,
        sensitivity_level: SensitivityLevel = SensitivityLevel.MEDIUM,
        server_status: ServerStatus = ServerStatus.ACTIVE,
        transport: TransportType = TransportType.HTTP
    ) -> MCPServer:
        """
        Generate a single MCP server with realistic data.
    
        Args:
            owner_id: User ID who owns the server
            team_id: Team ID the server belongs to
            sensitivity_level: Security sensitivity level
            is_active: Whether server is active
            transport: Transport protocol
    
        Returns:
            MCPServer object
        """
        server_types = [
            "api-gateway", "data-processor", "ml-model", "analytics",
            "monitoring", "logging", "auth-service", "notification"
        ]
    
        server_type = random.choice(server_types)
        server_name = f"{server_type}-{fake.word()}-{random.randint(1, 999)}"
    
        if not owner_id:
            owner_id = uuid4()
        if not team_id:
            team_id = uuid4()
    
        # Generate endpoint based on transport type
        if transport == TransportType.HTTP:
            endpoint = f"http://{server_name}.{fake.domain_name()}/mcp"
        elif transport == TransportType.STDIO:
            endpoint = f"/usr/local/bin/{server_name}"
        else:  # SSE
            endpoint = f"http://{server_name}.{fake.domain_name()}/events"
    
        return MCPServer(
            id=uuid4(),
            name=server_name,
            description=fake.sentence(),
            transport=transport,
            endpoint=endpoint,
            sensitivity_level=sensitivity_level,
            owner_id=owner_id,
            team_id=team_id,
>           is_active=is_active,
                      ^^^^^^^^^
            health_check_url=f"http://{server_name}.{fake.domain_name()}/health" if transport == TransportType.HTTP else None,
            created_at=datetime.now(UTC) - timedelta(days=random.randint(1, 180)),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'is_active' is not defined

tests/e2e/test_data_generator.py:170: NameError
_____________________________ test_generate_policy _____________________________

    def test_generate_policy():
        """Test policy generation."""
>       policy = generate_policy()
                 ^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:622: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

policy_type = 'authorization', status = <PolicyStatus.DRAFT: 'draft'>

    def generate_policy(
        policy_type: str = "authorization",
        status: PolicyStatus = PolicyStatus.DRAFT,
        # No created_by field in Policy = None
    ) -> Policy:
        """
        Generate a policy with realistic data.
    
        Args:
            policy_type: Type of policy (authorization, validation, etc.)
            status: Policy status
            created_by: User ID who created the policy
    
        Returns:
            Policy object
        """
        policy_names = [
            "server-registration-policy",
            "tool-invocation-policy",
            "data-access-policy",
            "admin-action-policy",
            "team-collaboration-policy"
        ]
    
>       if not created_by:
               ^^^^^^^^^^
E       UnboundLocalError: cannot access local variable 'created_by' where it is not associated with a value

tests/e2e/test_data_generator.py:294: UnboundLocalError
__________________________ test_generate_audit_event ___________________________

    def test_generate_audit_event():
        """Test audit event generation."""
>       event = generate_audit_event()
                ^^^^^^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:630: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/test_data_generator.py:401: in generate_audit_event
    return AuditEvent(
<string>:4: in __init__
    ???
/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state.py:571: in _initialize_instance
    with util.safe_reraise():
/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state.py:569: in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AuditEvent(id=26e34c28-224d-4dcd-bf30-64b3478a8665, type=AuditEventType.SERVER_REGISTERED, timestamp=None, severity=SeverityLevel.LOW)>
kwargs = {'action': 'register', 'details': {'correlation_id': '8909b459-ca93-458b-802d-6f2fbdee7288', 'message': 'Budget teacher cup play goal relationship.'}, 'event_type': <AuditEventType.SERVER_REGISTERED: 'server_registered'>, 'id': UUID('26e34c28-224d-4dcd-bf30-64b3478a8665'), ...}
cls_ = <class 'sark.models.audit.AuditEvent'>, k = 'resource_id'

    def _declarative_constructor(self: Any, **kwargs: Any) -> None:
        """A simple constructor that allows initialization from kwargs.
    
        Sets attributes on the constructed instance using the names and
        values in ``kwargs``.
    
        Only keys that are present as
        attributes of the instance's class are allowed. These could be,
        for example, any mapped columns or relationships.
        """
        cls_ = type(self)
        for k in kwargs:
            if not hasattr(cls_, k):
>               raise TypeError(
                    "%r is an invalid keyword argument for %s" % (k, cls_.__name__)
                )
E               TypeError: 'resource_id' is an invalid keyword argument for AuditEvent

/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/decl_base.py:2179: TypeError
__________________________ test_generate_audit_trail ___________________________

    def test_generate_audit_trail():
        """Test audit trail generation."""
        user_id = uuid4()
>       trail = generate_audit_trail(user_id, event_count=5)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/e2e/test_data_generator.py:640: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/test_data_generator.py:446: in generate_audit_trail
    event = generate_audit_event(
tests/e2e/test_data_generator.py:401: in generate_audit_event
    return AuditEvent(
<string>:4: in __init__
    ???
/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state.py:571: in _initialize_instance
    with util.safe_reraise():
/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/state.py:569: in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AuditEvent(id=118b86fc-8844-42b1-a1cf-7eb1c5e397d5, type=AuditEventType.USER_LOGIN, timestamp=None, severity=SeverityLevel.LOW)>
kwargs = {'action': 'login', 'details': {'correlation_id': 'cc4d322d-378c-4a89-afd7-93473f3935ba', 'message': 'Level help leader energy.'}, 'event_type': <AuditEventType.USER_LOGIN: 'user_login'>, 'id': UUID('118b86fc-8844-42b1-a1cf-7eb1c5e397d5'), ...}
cls_ = <class 'sark.models.audit.AuditEvent'>, k = 'resource_id'

    def _declarative_constructor(self: Any, **kwargs: Any) -> None:
        """A simple constructor that allows initialization from kwargs.
    
        Sets attributes on the constructed instance using the names and
        values in ``kwargs``.
    
        Only keys that are present as
        attributes of the instance's class are allowed. These could be,
        for example, any mapped columns or relationships.
        """
        cls_ = type(self)
        for k in kwargs:
            if not hasattr(cls_, k):
>               raise TypeError(
                    "%r is an invalid keyword argument for %s" % (k, cls_.__name__)
                )
E               TypeError: 'resource_id' is an invalid keyword argument for AuditEvent

/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/decl_base.py:2179: TypeError
_______________________ test_generate_realistic_dataset ________________________

    def test_generate_realistic_dataset():
        """Test complete dataset generation."""
>       dataset = generate_realistic_dataset(
            user_count=50,
            team_count=5,
            server_count=100,
            policy_count=10,
            audit_event_count=200
        )

tests/e2e/test_data_generator.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/e2e/test_data_generator.py:496: in generate_realistic_dataset
    user = generate_user(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

role = 'developer', is_admin = False, status = <ServerStatus.ACTIVE: 'active'>
team_id = UUID('33a3cc0c-f890-4bf6-b658-6273cf123245')

    def generate_user(
        role: str = "developer",
        is_admin: bool = False,
        status: ServerStatus = ServerStatus.ACTIVE,
        team_id: Optional[uuid4] = None
    ) -> User:
        """
        Generate a single user with realistic data.
    
        Args:
            role: User role (developer, analyst, admin)
            is_admin: Whether user has admin privileges
            is_active: Whether user account is active
            team_id: Optional team ID for user metadata
    
        Returns:
            User object with realistic data
        """
        first_name = fake.first_name()
        last_name = fake.last_name()
        email = f"{first_name.lower()}.{last_name.lower()}@{fake.domain_name()}"
    
        extra_metadata = {}
        if team_id:
            extra_metadata["team_id"] = str(team_id)
    
        return User(
            id=uuid4(),
            email=email,
            full_name=f"{first_name} {last_name}",
            hashed_password=fake.sha256(),
            role=role,
>           is_active=is_active,
                      ^^^^^^^^^
            is_admin=is_admin,
            extra_metadata=extra_metadata,
            created_at=datetime.now(UTC) - timedelta(days=random.randint(1, 365)),
            updated_at=datetime.now(UTC)
        )
E       NameError: name 'is_active' is not defined

tests/e2e/test_data_generator.py:61: NameError
___________________________ test_complete_login_flow ___________________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8614c50>
test_user = <User(id=581d025c-7ac5-462e-9a91-5086e397b86d, email=test@example.com, role=developer)>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_complete_login_flow(jwt_handler, test_user):
        """Test complete login flow with token generation."""
        # Generate access and refresh tokens
>       access_token = jwt_handler.create_access_token(user_id=test_user.id)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_auth_integration.py:102: TypeError
___________________________ test_token_refresh_flow ____________________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e8ced110>
test_user = <User(id=3a451598-2db8-4677-8456-357a7dde4157, email=test@example.com, role=developer)>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_token_refresh_flow(jwt_handler, test_user):
        """Test token refresh mechanism."""
        # Create initial tokens
>       old_access_token = jwt_handler.create_access_token(user_id=test_user.id)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'

tests/integration/test_auth_integration.py:123: TypeError
_______________________ test_logout_invalidates_session ________________________

session_service = <sark.services.auth.session.SessionService object at 0x7ee6e8663e90>
test_user = <User(id=3f4cf995-a4ac-4bee-98d2-36c2fb46d7fa, email=test@example.com, role=developer)>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_logout_invalidates_session(session_service, test_user):
        """Test that logout properly invalidates session."""
        # Create session
        session = session_service.create_session(
            user_id=test_user.id,
            user_agent="Mozilla/5.0",
            ip_address="127.0.0.1"
        )
    
        assert session.is_active is True
    
        # Invalidate session (logout)
>       invalidated = session_service.invalidate_session(session.session_id)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/test_auth_integration.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sark.services.auth.session.SessionService object at 0x7ee6e8663e90>
session = 'N52VX8vFx5xQ1g0Zbs270y-tPh7dROkYOhLTL9Iy9xQ'

    def invalidate_session(self, session: Session) -> Session:
        """
        Invalidate a session.
    
        Args:
            session: Session to invalidate
    
        Returns:
            Invalidated session
        """
>       session.is_active = False
        ^^^^^^^^^^^^^^^^^
E       AttributeError: 'str' object has no attribute 'is_active'

src/sark/services/auth/session.py:158: AttributeError
----------------------------- Captured stdout call -----------------------------
2025-11-23 01:23:26 [info     ] session_created                expires_at=2025-11-24T01:23:26.189304+00:00 session_id=N52VX8vFx5xQ1g0Zbs270y-tPh7dROkYOhLTL9Iy9xQ user_id=3f4cf995-a4ac-4bee-98d2-36c2fb46d7fa
______________________ test_invalid_credentials_rejected _______________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87c7150>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_invalid_credentials_rejected(jwt_handler):
        """Test that invalid tokens are rejected."""
        invalid_token = "invalid.token.here"
    
        with pytest.raises(HTTPException) as exc_info:
>           jwt_handler.verify_token(invalid_token, "access")
            ^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'JWTHandler' object has no attribute 'verify_token'

tests/integration/test_auth_integration.py:173: AttributeError
_________________________ test_expired_token_rejected __________________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e871ded0>
test_user = <User(id=b2b6a59f-23a6-4e9e-a2cc-4a4f7ae01c2a, email=test@example.com, role=developer)>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_expired_token_rejected(jwt_handler, test_user):
        """Test that expired tokens are rejected."""
        # Create expired token
        now = datetime.now(UTC)
        past = now - timedelta(hours=1)
        claims = {
            "sub": str(test_user.id),
            "exp": past,
            "type": "access"
        }
        expired_token = jwt.encode(claims, jwt_handler.secret_key, algorithm=jwt_handler.algorithm)
    
        with pytest.raises(HTTPException) as exc_info:
>           jwt_handler.verify_token(expired_token, "access")
            ^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'JWTHandler' object has no attribute 'verify_token'

tests/integration/test_auth_integration.py:194: AttributeError
________________________ test_wrong_token_type_rejected ________________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e870d790>
test_user = <User(id=9ad3d6d4-e7b6-44e9-aa7c-ea5809bc589a, email=test@example.com, role=developer)>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_wrong_token_type_rejected(jwt_handler, test_user):
        """Test that using refresh token for access fails."""
>       refresh_token = jwt_handler.create_refresh_token(user_id=test_user.id)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: JWTHandler.create_refresh_token() missing 1 required positional argument: 'email'

tests/integration/test_auth_integration.py:204: TypeError
________________________ test_malformed_token_rejected _________________________

jwt_handler = <sark.services.auth.jwt.JWTHandler object at 0x7ee6e87d7090>

    @pytest.mark.asyncio
    @pytest.mark.integration
    @pytest.mark.auth
    async def test_malformed_token_rejected(jwt_handler):
        """Test that malformed tokens are rejected."""
        malformed_tokens = [
            "",
            "not-a-jwt",
            "header.payload",  # Missing signature
            "header.payload.signature.extra"  # Too many parts
        ]
    
        for token in malformed_tokens:
            with pytest.raises(HTTPException) as exc_info:
>               jwt_handler.verify_token(token, "access")
                ^^^^^^^^^^^^^^^^^^^^^^^^
E               AttributeError: 'JWTHandler' object has no attribute 'verify_token'

tests/integration/test_auth_integration.py:227: AttributeError
________ TestServerListPagination.test_list_servers_default_pagination _________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906d6d0>
client = <starlette.testclient.TestClient object at 0x7ee6e7ad1810>

    def test_list_servers_default_pagination(self, client: TestClient) -> None:
        """Test listing servers with default pagination parameters."""
        # Mock the database and discovery service
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            # Create mock discovery service instance
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            # Mock paginated response
            mock_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(50)
            ]
    
            mock_instance.list_servers_paginated = AsyncMock(
                return_value=(mock_servers, "next_cursor_123", True, None)
            )
    
            # Make request
            response = client.get("/api/servers/")
    
            # Verify response
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:77: AssertionError
___________ TestServerListPagination.test_list_servers_custom_limit ____________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906dd50>
client = <starlette.testclient.TestClient object at 0x7ee6e7862890>

    def test_list_servers_custom_limit(self, client: TestClient) -> None:
        """Test listing servers with custom page limit."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            mock_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(10)
            ]
    
            mock_instance.list_servers_paginated = AsyncMock(
                return_value=(mock_servers, None, False, None)
            )
    
            # Make request with custom limit
            response = client.get("/api/servers/?limit=10")
    
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:112: AssertionError
____________ TestServerListPagination.test_list_servers_with_cursor ____________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906e3d0>
client = <starlette.testclient.TestClient object at 0x7ee6e76a1250>

    def test_list_servers_with_cursor(self, client: TestClient) -> None:
        """Test listing servers with pagination cursor."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            mock_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(50)
            ]
    
            mock_instance.list_servers_paginated = AsyncMock(
                return_value=(mock_servers, "next_cursor_456", True, None)
            )
    
            # Make request with cursor
            response = client.get("/api/servers/?cursor=cursor_123")
    
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:144: AssertionError
__________ TestServerListPagination.test_list_servers_ascending_order __________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906ea90>
client = <starlette.testclient.TestClient object at 0x7ee6e7b2a4d0>

    def test_list_servers_ascending_order(self, client: TestClient) -> None:
        """Test listing servers with ascending sort order."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            mock_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(25)
            ]
    
            mock_instance.list_servers_paginated = AsyncMock(
                return_value=(mock_servers, None, False, None)
            )
    
            # Make request with ascending sort
            response = client.get("/api/servers/?sort_order=asc")
    
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:175: AssertionError
____________ TestServerListPagination.test_list_servers_with_total _____________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906f110>
client = <starlette.testclient.TestClient object at 0x7ee6e7ab3550>

    def test_list_servers_with_total(self, client: TestClient) -> None:
        """Test listing servers with total count."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            mock_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(50)
            ]
    
            mock_instance.list_servers_paginated = AsyncMock(
                return_value=(mock_servers, "cursor_789", True, 250)
            )
    
            # Make request with total count
            response = client.get("/api/servers/?include_total=true")
    
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:205: AssertionError
_______ TestServerListPagination.test_list_servers_invalid_limit_too_low _______

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906f790>
client = <starlette.testclient.TestClient object at 0x7ee6e76e2190>

    def test_list_servers_invalid_limit_too_low(self, client: TestClient) -> None:
        """Test listing servers with invalid limit (too low)."""
        response = client.get("/api/servers/?limit=0")
    
>       assert response.status_code == 422  # Validation error
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 404 == 422
E        +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:215: AssertionError
______ TestServerListPagination.test_list_servers_invalid_limit_too_high _______

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906fe10>
client = <starlette.testclient.TestClient object at 0x7ee6e7726210>

    def test_list_servers_invalid_limit_too_high(self, client: TestClient) -> None:
        """Test listing servers with invalid limit (too high)."""
        response = client.get("/api/servers/?limit=201")
    
>       assert response.status_code == 422  # Validation error
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 404 == 422
E        +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:221: AssertionError
________ TestServerListPagination.test_list_servers_invalid_sort_order _________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e906ec10>
client = <starlette.testclient.TestClient object at 0x7ee6e7739810>

    def test_list_servers_invalid_sort_order(self, client: TestClient) -> None:
        """Test listing servers with invalid sort order."""
        response = client.get("/api/servers/?sort_order=invalid")
    
>       assert response.status_code == 422  # Validation error
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 404 == 422
E        +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:227: AssertionError
___________ TestServerListPagination.test_list_servers_empty_results ___________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e905ac90>
client = <starlette.testclient.TestClient object at 0x7ee6e76e1250>

    def test_list_servers_empty_results(self, client: TestClient) -> None:
        """Test listing servers with no results."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            mock_instance.list_servers_paginated = AsyncMock(return_value=([], None, False, 0))
    
            response = client.get("/api/servers/")
    
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:239: AssertionError
__________ TestServerListPagination.test_list_servers_response_schema __________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e9080410>
client = <starlette.testclient.TestClient object at 0x7ee6e771f610>

    def test_list_servers_response_schema(self, client: TestClient) -> None:
        """Test that server list response matches expected schema."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            test_id = uuid4()
            test_time = datetime.now(UTC)
    
            mock_server = MagicMock(
                id=test_id,
                name="test-server",
                transport=TransportType.HTTP,
                status=ServerStatus.ACTIVE,
                sensitivity_level=SensitivityLevel.HIGH,
                created_at=test_time,
            )
    
            mock_instance.list_servers_paginated = AsyncMock(
                return_value=([mock_server], None, False, None)
            )
    
            response = client.get("/api/servers/")
    
>           assert response.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:270: AssertionError
__________ TestServerListPagination.test_list_servers_multiple_pages ___________

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e90807d0>
client = <starlette.testclient.TestClient object at 0x7ee6e7b85b90>

    def test_list_servers_multiple_pages(self, client: TestClient) -> None:
        """Test pagination across multiple pages."""
        with patch("sark.api.routers.servers.DiscoveryService") as mock_service:
            mock_instance = MagicMock()
            mock_service.return_value = mock_instance
    
            # First page - 50 items with cursor
            first_page_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(50)
            ]
    
            # Second page - 30 items, no more pages
            second_page_servers = [
                MagicMock(
                    id=uuid4(),
                    name=f"server-{i}",
                    transport=TransportType.HTTP,
                    status=ServerStatus.ACTIVE,
                    sensitivity_level=SensitivityLevel.MEDIUM,
                    created_at=datetime.now(UTC),
                )
                for i in range(50, 80)
            ]
    
            # Configure mock to return different results for different calls
            mock_instance.list_servers_paginated = AsyncMock(
                side_effect=[
                    (first_page_servers, "cursor_page2", True, None),
                    (second_page_servers, None, False, None),
                ]
            )
    
            # Get first page
            response1 = client.get("/api/servers/")
>           assert response1.status_code == 200
E           assert 404 == 200
E            +  where 404 = <Response [404 Not Found]>.status_code

tests/test_api_pagination.py:330: AssertionError
_______ TestServerListPagination.test_openapi_schema_includes_pagination _______

self = <tests.test_api_pagination.TestServerListPagination object at 0x7ee6e9080b90>
client = <starlette.testclient.TestClient object at 0x7ee6e78aaed0>

    def test_openapi_schema_includes_pagination(self, client: TestClient) -> None:
        """Test that OpenAPI schema includes pagination models."""
        response = client.get("/openapi.json")
    
        assert response.status_code == 200
        schema = response.json()
    
        # Verify pagination query parameters are documented
        servers_path = schema["paths"].get("/api/servers/", {}).get("get", {})
        parameters = servers_path.get("parameters", [])
    
        param_names = [p["name"] for p in parameters]
>       assert "limit" in param_names
E       AssertionError: assert 'limit' in []

tests/test_api_pagination.py:358: AssertionError
=============================== warnings summary ===============================
src/sark/db/base.py:5
  /home/user/sark/src/sark/db/base.py:5: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base = declarative_base()

src/sark/config/settings.py:104
  /home/user/sark/src/sark/config/settings.py:104: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("cors_origins", pre=True)

src/sark/config/settings.py:111
  /home/user/sark/src/sark/config/settings.py:111: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("kafka_bootstrap_servers", pre=True)

tests/test_pagination.py:17
  /home/user/sark/tests/test_pagination.py:17: PytestCollectionWarning: cannot collect test class 'TestModel' because it has a __init__ constructor (from: tests/test_pagination.py)
    class TestModel(Base):

tests/test_api.py::test_health_check
  /home/user/sark/src/sark/api/main.py:45: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")

tests/test_api.py::test_health_check
tests/test_api.py::test_health_check
  /usr/local/lib/python3.11/dist-packages/fastapi/applications.py:4575: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    return self.router.on_event(event_type)

tests/test_api.py::test_health_check
  /home/user/sark/src/sark/api/main.py:60: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("shutdown")

tests/test_bulk_operations.py::TestBulkOperationsService::test_bulk_register_transactional_rollback_on_error
  /home/user/sark/src/sark/services/bulk/__init__.py:153: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    async with self.db.begin_nested():
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_pagination.py::TestCursorPaginator::test_paginate_with_total_count
  /home/user/sark/src/sark/api/pagination.py:161: SADeprecationWarning: The Select.froms attribute is moved to the Select.get_final_froms() method. (deprecated since: 1.4.23)
    count_query = select(func.count()).select_from(query.froms[0])

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.11.14-final-0 _______________

Name                                               Stmts   Miss Branch BrPart   Cover   Missing
-----------------------------------------------------------------------------------------------
src/sark/__init__.py                                   4      0      0      0 100.00%
src/sark/__main__.py                                   4      4      0      0   0.00%   3-10
src/sark/api/__init__.py                               0      0      0      0 100.00%
src/sark/api/main.py                                  29      4      2      1  83.87%   41->45, 48-58, 63
src/sark/api/middleware/__init__.py                    2      2      0      0   0.00%   3-5
src/sark/api/pagination.py                            66      1     16      1  97.56%   125
src/sark/api/routers/__init__.py                       0      0      0      0 100.00%
src/sark/api/routers/bulk.py                          63     22      2      0  63.08%   135-174, 235-269
src/sark/api/routers/health.py                       107     38      0      0  64.49%   78, 125, 140-197, 224-227, 251-253, 278-280
src/sark/api/routers/policy.py                        33     11      0      0  66.67%   49-91
src/sark/api/routers/servers.py                       81     29      4      0  61.18%   85-154, 166-178, 234-276
src/sark/cache.py                                    135    135     22      0   0.00%   9-362
src/sark/config.py                                   119    119     30      0   0.00%   11-251
src/sark/config/__init__.py                            2      0      0      0 100.00%
src/sark/config/settings.py                           84      3      4      2  94.32%   108, 115, 129
src/sark/database.py                                 125    125     26      0   0.00%   8-287
src/sark/db/__init__.py                                3      0      0      0 100.00%
src/sark/db/base.py                                    2      0      0      0 100.00%
src/sark/db/session.py                                54     39      8      0  24.19%   20-28, 34-42, 48-54, 60-66, 71-80, 85-94, 99-108
src/sark/health.py                                    34     34      6      0   0.00%   3-133
src/sark/kong_client.py                              105    105     16      0   0.00%   8-296
src/sark/logging_config.py                            37     37     12      0   0.00%   3-99
src/sark/main.py                                      34     34      6      0   0.00%   3-101
src/sark/metrics.py                                   40     40      4      0   0.00%   3-141
src/sark/models/__init__.py                            5      0      0      0 100.00%
src/sark/models/audit.py                              42      0      0      0 100.00%
src/sark/models/mcp_server.py                         64      0      0      0 100.00%
src/sark/models/policy.py                             41      0      0      0 100.00%
src/sark/models/user.py                               31      0      0      0 100.00%
src/sark/services/__init__.py                          0      0      0      0 100.00%
src/sark/services/audit/__init__.py                    2      0      0      0 100.00%
src/sark/services/audit/audit_service.py              33      0      2      0 100.00%
src/sark/services/auth/__init__.py                     6      0      0      0 100.00%
src/sark/services/auth/api_key.py                     76      0     16      1  98.91%   231->230
src/sark/services/auth/jwt.py                         66      0      6      0 100.00%
src/sark/services/auth/providers/__init__.py           5      0      0      0 100.00%
src/sark/services/auth/providers/base.py              28      5      0      0  82.14%   99-104
src/sark/services/auth/providers/ldap.py              61      6      6      0  91.04%   142-144, 224-226
src/sark/services/auth/providers/oidc.py              69      3      6      0  96.00%   130-132
src/sark/services/auth/providers/saml.py              92      4     18      3  93.64%   187->191, 194->198, 206, 269-271
src/sark/services/auth/session.py                     97      0     18      2  98.26%   266->268, 299->298
src/sark/services/auth/user_context.py                20      0      0      0 100.00%
src/sark/services/bulk/__init__.py                   143      0     26      0 100.00%
src/sark/services/discovery/__init__.py                2      0      0      0 100.00%
src/sark/services/discovery/discovery_service.py      84      0     16      0 100.00%
src/sark/services/discovery/search.py                121      0     54      2  98.86%   70->75, 154->157
src/sark/services/policy/__init__.py                   3      0      0      0 100.00%
src/sark/services/policy/opa_client.py                59      0      0      0 100.00%
src/sark/services/policy/policy_service.py            58      0      8      0 100.00%
-----------------------------------------------------------------------------------------------
TOTAL                                               2371    800    334     12  64.95%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/e2e/test_complete_flows.py::test_complete_user_registration_to_server_management - NameError: name 'ServerStatus' is not defined
FAILED tests/e2e/test_complete_flows.py::test_admin_policy_creation_to_enforcement - NameError: name 'ServerStatus' is not defined
FAILED tests/e2e/test_complete_flows.py::test_multi_team_server_sharing - NameError: name 'ServerStatus' is not defined
FAILED tests/e2e/test_complete_flows.py::test_complete_audit_trail_flow - NameError: name 'ServerStatus' is not defined
FAILED tests/e2e/test_data_generator.py::test_generate_user - NameError: name 'is_active' is not defined
FAILED tests/e2e/test_data_generator.py::test_generate_users - NameError: name 'is_active' is not defined
FAILED tests/e2e/test_data_generator.py::test_generate_mcp_server - NameError: name 'is_active' is not defined
FAILED tests/e2e/test_data_generator.py::test_generate_servers - NameError: name 'is_active' is not defined
FAILED tests/e2e/test_data_generator.py::test_generate_server_with_tools - NameError: name 'is_active' is not defined
FAILED tests/e2e/test_data_generator.py::test_generate_policy - UnboundLocalError: cannot access local variable 'created_by' where it is not associated with a value
FAILED tests/e2e/test_data_generator.py::test_generate_audit_event - TypeError: 'resource_id' is an invalid keyword argument for AuditEvent
FAILED tests/e2e/test_data_generator.py::test_generate_audit_trail - TypeError: 'resource_id' is an invalid keyword argument for AuditEvent
FAILED tests/e2e/test_data_generator.py::test_generate_realistic_dataset - NameError: name 'is_active' is not defined
FAILED tests/integration/test_auth_integration.py::test_complete_login_flow - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
FAILED tests/integration/test_auth_integration.py::test_token_refresh_flow - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
FAILED tests/integration/test_auth_integration.py::test_logout_invalidates_session - AttributeError: 'str' object has no attribute 'is_active'
FAILED tests/integration/test_auth_integration.py::test_invalid_credentials_rejected - AttributeError: 'JWTHandler' object has no attribute 'verify_token'
FAILED tests/integration/test_auth_integration.py::test_expired_token_rejected - AttributeError: 'JWTHandler' object has no attribute 'verify_token'
FAILED tests/integration/test_auth_integration.py::test_wrong_token_type_rejected - TypeError: JWTHandler.create_refresh_token() missing 1 required positional argument: 'email'
FAILED tests/integration/test_auth_integration.py::test_malformed_token_rejected - AttributeError: 'JWTHandler' object has no attribute 'verify_token'
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_default_pagination - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_custom_limit - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_with_cursor - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_ascending_order - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_with_total - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_invalid_limit_too_low - assert 404 == 422
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_invalid_limit_too_high - assert 404 == 422
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_invalid_sort_order - assert 404 == 422
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_empty_results - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_response_schema - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_list_servers_multiple_pages - assert 404 == 200
 +  where 404 = <Response [404 Not Found]>.status_code
FAILED tests/test_api_pagination.py::TestServerListPagination::test_openapi_schema_includes_pagination - AssertionError: assert 'limit' in []
ERROR tests/integration/test_api_integration.py::test_register_server_endpoint - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_register_server_validates_input - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_get_server_by_id - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_update_server_endpoint - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_delete_server_endpoint - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_search_servers_by_name - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_filter_servers_by_status - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_filter_servers_by_sensitivity - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_pagination_first_page - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_pagination_last_page - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_pagination_empty_results - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_bulk_register_servers_transactional - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_bulk_register_servers_best_effort - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_bulk_operation_rollback_on_error - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_404_for_nonexistent_server - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_validation_error_response - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_forbidden_access_response - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_pagination_performance_consistency - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_pagination_edge_cases - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_search_performance_with_large_dataset - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_multi_filter_performance - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_complex_search_filters - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_bulk_register_best_effort_mode - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_bulk_update_servers - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_bulk_delete_servers - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_admin_only_endpoints_require_admin_role - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_malformed_json_rejected - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_missing_required_fields - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_invalid_field_types - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_field_length_validation - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_duplicate_name_rejected - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_invalid_sensitivity_level - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_sql_injection_attempts_blocked - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_xss_attempts_sanitized - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_api_latency_p95_under_100ms - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_api_integration.py::test_concurrent_request_handling - TypeError: JWTHandler.create_access_token() missing 2 required positional arguments: 'email' and 'role'
ERROR tests/integration/test_auth_integration.py::test_opa_authorization_allows_valid_request - pytest.PytestRemovedIn9Warning: 'test_opa_authorization_allows_valid_request' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_auth_integration.py::test_opa_authorization_denies_invalid_request - pytest.PytestRemovedIn9Warning: 'test_opa_authorization_denies_invalid_request' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_auth_integration.py::test_opa_fail_closed_on_error - pytest.PytestRemovedIn9Warning: 'test_opa_fail_closed_on_error' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_auth_integration.py::test_admin_has_elevated_permissions - pytest.PytestRemovedIn9Warning: 'test_admin_has_elevated_permissions' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_auth_integration.py::test_regular_user_lacks_admin_permissions - pytest.PytestRemovedIn9Warning: 'test_regular_user_lacks_admin_permissions' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_allows_server_registration - pytest.PytestRemovedIn9Warning: 'test_policy_allows_server_registration' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_denies_unauthorized_registration - pytest.PytestRemovedIn9Warning: 'test_policy_denies_unauthorized_registration' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_enforces_sensitivity_levels - pytest.PytestRemovedIn9Warning: 'test_policy_enforces_sensitivity_levels' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_allows_tool_invocation - pytest.PytestRemovedIn9Warning: 'test_policy_allows_tool_invocation' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_denies_dangerous_tool_invocation - pytest.PytestRemovedIn9Warning: 'test_policy_denies_dangerous_tool_invocation' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_evaluates_bulk_operations - pytest.PytestRemovedIn9Warning: 'test_policy_evaluates_bulk_operations' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_fails_bulk_on_single_denial - pytest.PytestRemovedIn9Warning: 'test_policy_fails_bulk_on_single_denial' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_denial_provides_reason - pytest.PytestRemovedIn9Warning: 'test_policy_denial_provides_reason' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_policy_handles_opa_errors_gracefully - pytest.PytestRemovedIn9Warning: 'test_policy_handles_opa_errors_gracefully' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_create_multiple_policy_versions - pytest.PytestRemovedIn9Warning: 'test_create_multiple_policy_versions' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_activate_policy_version - pytest.PytestRemovedIn9Warning: 'test_activate_policy_version' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_fail_closed_on_network_error - pytest.PytestRemovedIn9Warning: 'test_fail_closed_on_network_error' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_fail_closed_on_invalid_response - pytest.PytestRemovedIn9Warning: 'test_fail_closed_on_invalid_response' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_policy_integration.py::test_fail_closed_on_missing_result - pytest.PytestRemovedIn9Warning: 'test_fail_closed_on_missing_result' requested an async fixture 'opa_client', with no plugin or hook that handled it. This is usually an error, as pytest does not natively support it. This will turn into an error in pytest 9.
See: https://docs.pytest.org/en/stable/deprecations.html#sync-test-depending-on-async-fixture
ERROR tests/integration/test_siem_integration.py::test_audit_event_persisted_to_database - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_audit_events_include_metadata - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_high_severity_events_forwarded_to_splunk - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_low_severity_events_not_forwarded_to_siem - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_events_routed_by_severity - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_events_filtered_by_type - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_continues_on_siem_failure - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_handles_siem_authentication_failure - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_handles_siem_network_timeout - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_audit_trail_captures_all_operations - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
ERROR tests/integration/test_siem_integration.py::test_audit_events_include_correlation_data - TypeError: AuditService.__init__() got an unexpected keyword argument 'siem_enabled'
==== 32 failed, 557 passed, 26 deselected, 10 warnings, 66 errors in 18.05s ====
